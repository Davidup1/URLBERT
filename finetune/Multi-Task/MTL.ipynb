{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "import random\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForMaskedLM,\n",
    ")\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def dataPreprocessFromCSV(filename, input_ids, input_types, input_masks, label, task_label, task='Phish'):\n",
    "    pad_size = 200\n",
    "    tokenizer = BertTokenizer(\"path_to_the_vocab\")  # Initialize the tokenizer\n",
    "    data = pd.read_csv(filename, encoding='utf-8')\n",
    "    for i, row in tqdm(data.iterrows(), total=len(data)):\n",
    "        x1 = row['url']  # Replace with the column name in your CSV file where the text data is located\n",
    "        x1 = tokenizer.tokenize(x1)\n",
    "        tokens = [\"[CLS]\"] + x1 + [\"[SEP]\"]\n",
    "        # Get input_id, seg_id, att_mask\n",
    "        ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        types = [0] * (len(ids))\n",
    "        masks = [1] * len(ids)\n",
    "        # Pad if short, truncate if long\n",
    "        if len(ids) < pad_size:\n",
    "            types = types + [1] * (pad_size - len(ids))  # Set segment to 1 for the masked part\n",
    "            masks = masks + [0] * (pad_size - len(ids))\n",
    "            ids = ids + [0] * (pad_size - len(ids))\n",
    "        else:\n",
    "            types = types[:pad_size]\n",
    "            masks = masks[:pad_size]\n",
    "            ids = ids[:pad_size]\n",
    "        input_ids.append(ids)\n",
    "        input_types.append(types)\n",
    "        input_masks.append(masks)\n",
    "        assert len(ids) == len(masks) == len(types) == pad_size\n",
    "        if task == 'Phish':\n",
    "            y = row['label']\n",
    "            if y == 'malicious':\n",
    "                label.append([1])\n",
    "                task_label.append([0])\n",
    "            elif y == 'benign':\n",
    "                label.append([0])\n",
    "                task_label.append([0])\n",
    "        elif task == 'Multi':\n",
    "            y = row['label']\n",
    "            if y == 'Games':\n",
    "                label.append([0])\n",
    "                task_label.append([1])\n",
    "            elif y == 'Health':\n",
    "                label.append([1])\n",
    "                task_label.append([1])\n",
    "            elif y == 'Kids':\n",
    "                label.append([2])\n",
    "                task_label.append([1])\n",
    "            elif y == 'Reference':\n",
    "                label.append([3])\n",
    "                task_label.append([1])\n",
    "            elif y == 'Shopping':\n",
    "                label.append([4])\n",
    "                task_label.append([1])\n",
    "        elif task == 'Advertise':\n",
    "            y = row['label']\n",
    "            if y == 'white':\n",
    "                label.append([0])\n",
    "                task_label.append([2])\n",
    "            elif y == 'advertise':\n",
    "                label.append([1])\n",
    "                task_label.append([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a910f807a5e141b9aefb2506f3df5003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/639999 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b951098d988b4fd1a3d8c9c22dd4cfc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/316273 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "086b3886e16644beb47de1dc1ee4a93c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47060 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_ids = {\"Phish\":[], \"Multi\":[], \"Advertise\":[]}  # input char ids\n",
    "input_types = {\"Phish\":[], \"Multi\":[], \"Advertise\":[]}  # segment ids\n",
    "input_masks = {\"Phish\":[], \"Multi\":[], \"Advertise\":[]}  # attention mask\n",
    "label = {\"Phish\":[], \"Multi\":[], \"Advertise\":[]}\n",
    "task_label = {\"Phish\":[], \"Multi\":[], \"Advertise\":[]}\n",
    "dataset_root = {\"Phish\":\"path_to_the_dataset\", \"Multi\":\"path_to_the_dataset\", \"Advertise\":\"path_to_the_dataset\"}\n",
    "\n",
    "dataPreprocessFromCSV(dataset_root[\"Phish\"], input_ids[\"Phish\"], input_types[\"Phish\"], input_masks[\"Phish\"], label[\"Phish\"], task_label[\"Phish\"], 'Phish')\n",
    "dataPreprocessFromCSV(dataset_root[\"Multi\"], input_ids[\"Multi\"], input_types[\"Multi\"], input_masks[\"Multi\"], label[\"Multi\"], task_label[\"Multi\"], 'Multi')\n",
    "dataPreprocessFromCSV(dataset_root[\"Advertise\"], input_ids[\"Advertise\"], input_types[\"Advertise\"], input_masks[\"Advertise\"], label[\"Advertise\"], task_label[\"Advertise\"], 'Advertise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def spiltDatast_bert(input_ids, input_types, input_masks, label, task_label):\n",
    "    # Randomly shuffle the indices\n",
    "    random_order = list(range(len(input_ids)))\n",
    "    np.random.seed(2024)  # Fix the seed\n",
    "    np.random.shuffle(random_order)\n",
    "    print(random_order[:10])\n",
    "\n",
    "    # Split the dataset into 80% training and 20% testing\n",
    "    input_ids_train = np.array([input_ids[i] for i in random_order[:int(len(input_ids) * 0.8)]])\n",
    "    input_types_train = np.array([input_types[i] for i in random_order[:int(len(input_ids) * 0.8)]])\n",
    "    input_masks_train = np.array([input_masks[i] for i in random_order[:int(len(input_ids) * 0.8)]])\n",
    "    y_train = np.array([label[i] for i in random_order[:int(len(input_ids) * 0.8)]])\n",
    "    task_train = np.array([task_label[i] for i in random_order[:int(len(input_ids) * 0.8)]])\n",
    "    print(\"input_ids_train.shape:\" + str(input_ids_train.shape))\n",
    "    print(\"input_types_train.shape:\" + str(input_types_train.shape))\n",
    "    print(\"input_masks_train.shape:\" + str(input_masks_train.shape))\n",
    "    print(\"y_train.shape:\" + str(y_train.shape))\n",
    "    print(\"task_train.shape:\" + str(task_train.shape))\n",
    "\n",
    "    input_ids_test = np.array([input_ids[i] for i in random_order[int(len(input_ids) * 0.8):]])\n",
    "    input_types_test = np.array([input_types[i] for i in random_order[int(len(input_ids) * 0.8):]])\n",
    "    input_masks_test = np.array([input_masks[i] for i in random_order[int(len(input_ids) * 0.8):]])\n",
    "    y_test = np.array([label[i] for i in random_order[int(len(input_ids) * 0.8):]])\n",
    "    task_test = np.array([task_label[i] for i in random_order[int(len(input_ids) * 0.8):]])\n",
    "    print(\"input_ids_test.shape:\" + str(input_ids_test.shape))\n",
    "    print(\"input_types_test.shape:\" + str(input_types_test.shape))\n",
    "    print(\"input_masks_test.shape:\" + str(input_masks_test.shape))\n",
    "    print(\"y_test.shape:\" + str(y_test.shape))\n",
    "    print(\"task_test.shape:\" + str(task_test.shape))\n",
    "\n",
    "    return input_ids_train, input_types_train, input_masks_train, y_train, task_train, input_ids_test, input_types_test, input_masks_test, y_test, task_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[148401, 209465, 456, 205277, 265839, 410052, 504627, 261678, 179483, 12326]\n",
      "input_ids_train.shape:(511999, 200)\n",
      "input_types_train.shape:(511999, 200)\n",
      "input_masks_train.shape:(511999, 200)\n",
      "y_train.shape:(511999, 1)\n",
      "task_train.shape:(511999, 1)\n",
      "input_ids_test.shape:(128000, 200)\n",
      "input_types_test.shape:(128000, 200)\n",
      "input_masks_test.shape:(128000, 200)\n",
      "y_test.shape:(128000, 1)\n",
      "task_test.shape:(128000, 1)\n",
      "[142138, 126549, 271444, 181968, 282010, 101878, 31131, 165811, 70667, 191902]\n",
      "input_ids_train.shape:(253018, 200)\n",
      "input_types_train.shape:(253018, 200)\n",
      "input_masks_train.shape:(253018, 200)\n",
      "y_train.shape:(253018, 1)\n",
      "task_train.shape:(253018, 1)\n",
      "input_ids_test.shape:(63255, 200)\n",
      "input_types_test.shape:(63255, 200)\n",
      "input_masks_test.shape:(63255, 200)\n",
      "y_test.shape:(63255, 1)\n",
      "task_test.shape:(63255, 1)\n",
      "[41297, 28631, 7067, 32129, 43874, 13286, 7760, 39658, 45347, 930]\n",
      "input_ids_train.shape:(37648, 200)\n",
      "input_types_train.shape:(37648, 200)\n",
      "input_masks_train.shape:(37648, 200)\n",
      "y_train.shape:(37648, 1)\n",
      "task_train.shape:(37648, 1)\n",
      "input_ids_test.shape:(9412, 200)\n",
      "input_types_test.shape:(9412, 200)\n",
      "input_masks_test.shape:(9412, 200)\n",
      "y_test.shape:(9412, 1)\n",
      "task_test.shape:(9412, 1)\n"
     ]
    }
   ],
   "source": [
    "input_ids_train = {\"Phish\":[], \"Multi\":[], \"Advertise\":[]}\n",
    "input_types_train = {\"Phish\":[], \"Multi\":[], \"Advertise\":[]}\n",
    "input_masks_train = {\"Phish\":[], \"Multi\":[], \"Advertise\":[]}\n",
    "y_train = {\"Phish\":[], \"Multi\":[], \"Advertise\":[]}\n",
    "task_train = {\"Phish\":[], \"Multi\":[], \"Advertise\":[]}\n",
    "input_ids_val = {\"Phish\":[], \"Multi\":[], \"Advertise\":[]}\n",
    "input_types_val = {\"Phish\":[], \"Multi\":[], \"Advertise\":[]}\n",
    "input_masks_val = {\"Phish\":[], \"Multi\":[], \"Advertise\":[]}\n",
    "y_val = {\"Phish\":[], \"Multi\":[], \"Advertise\":[]}\n",
    "task_val = {\"Phish\":[], \"Multi\":[], \"Advertise\":[]}\n",
    "\n",
    "input_ids_train[\"Phish\"], input_types_train[\"Phish\"], input_masks_train[\"Phish\"], y_train[\"Phish\"], task_train[\"Phish\"], input_ids_val[\"Phish\"], input_types_val[\"Phish\"], input_masks_val[\"Phish\"], y_val[\"Phish\"], task_val[\"Phish\"] = spiltDatast_bert(\n",
    "    input_ids[\"Phish\"], input_types[\"Phish\"], input_masks[\"Phish\"], label[\"Phish\"], task_label[\"Phish\"])\n",
    "\n",
    "input_ids_train[\"Multi\"], input_types_train[\"Multi\"], input_masks_train[\"Multi\"], y_train[\"Multi\"], task_train[\"Multi\"], input_ids_val[\"Multi\"], input_types_val[\"Multi\"], input_masks_val[\"Multi\"], y_val[\"Multi\"], task_val[\"Multi\"] = spiltDatast_bert(\n",
    "    input_ids[\"Multi\"], input_types[\"Multi\"], input_masks[\"Multi\"], label[\"Multi\"], task_label[\"Multi\"])\n",
    "\n",
    "input_ids_train[\"Advertise\"], input_types_train[\"Advertise\"], input_masks_train[\"Advertise\"], y_train[\"Advertise\"], task_train[\"Advertise\"], input_ids_val[\"Advertise\"], input_types_val[\"Advertise\"], input_masks_val[\"Advertise\"], y_val[\"Advertise\"], task_val[\"Advertise\"] = spiltDatast_bert(\n",
    "    input_ids[\"Advertise\"], input_types[\"Advertise\"], input_masks[\"Advertise\"], label[\"Advertise\"], task_label[\"Advertise\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in input_ids_train:\n",
    "    if i == \"Advertise\":\n",
    "        input_ids_train[i] = input_ids_train[i][:len(input_ids_train[\"Advertise\"])]\n",
    "    else:\n",
    "        input_ids_train[i] = input_ids_train[i][:len(input_ids_train[\"Advertise\"]) * 3]\n",
    "for i in input_types_train:\n",
    "    if i == \"Advertise\":\n",
    "        input_types_train[i] = input_types_train[i][:len(input_ids_train[\"Advertise\"])]\n",
    "    else:\n",
    "        input_types_train[i] = input_types_train[i][:len(input_ids_train[\"Advertise\"]) * 3]\n",
    "for i in input_masks_train:\n",
    "    if i == \"Advertise\":\n",
    "        input_masks_train[i] = input_masks_train[i][:len(input_ids_train[\"Advertise\"])]\n",
    "    else:\n",
    "        input_masks_train[i] = input_masks_train[i][:len(input_ids_train[\"Advertise\"]) * 3]\n",
    "for i in y_train:\n",
    "    if i == \"Advertise\":\n",
    "        y_train[i] = y_train[i][:len(input_ids_train[\"Advertise\"])]\n",
    "    else:\n",
    "        y_train[i] = y_train[i][:len(input_ids_train[\"Advertise\"]) * 3]\n",
    "for i in task_train:\n",
    "    if i == \"Advertise\":\n",
    "        task_train[i] = task_train[i][:len(input_ids_train[\"Advertise\"])]\n",
    "    else:\n",
    "        task_train[i] = task_train[i][:len(input_ids_train[\"Advertise\"]) * 3]\n",
    "for i in input_ids_val:\n",
    "    if i == \"Advertise\":\n",
    "        input_ids_val[i] = input_ids_val[i][:len(input_ids_val[\"Advertise\"])]\n",
    "    else:\n",
    "        input_ids_val[i] = input_ids_val[i][:len(input_ids_val[\"Advertise\"]) * 3]\n",
    "for i in input_types_val:\n",
    "    if i == \"Advertise\":\n",
    "        input_types_val[i] = input_types_val[i][:len(input_ids_val[\"Advertise\"])]\n",
    "    else:\n",
    "        input_types_val[i] = input_types_val[i][:len(input_ids_val[\"Advertise\"]) * 3]\n",
    "for i in input_masks_val:\n",
    "    if i == \"Advertise\":\n",
    "        input_masks_val[i] = input_masks_val[i][:len(input_ids_val[\"Advertise\"])]\n",
    "    else:\n",
    "        input_masks_val[i] = input_masks_val[i][:len(input_ids_val[\"Advertise\"]) * 3]\n",
    "for i in y_val:\n",
    "    if i == \"Advertise\":\n",
    "        y_val[i] = y_val[i][:len(input_ids_val[\"Advertise\"])]\n",
    "    else:\n",
    "        y_val[i] = y_val[i][:len(input_ids_val[\"Advertise\"]) * 3]\n",
    "for i in task_val:\n",
    "    if i == \"Advertise\":\n",
    "        task_val[i] = task_val[i][:len(input_ids_val[\"Advertise\"])]\n",
    "    else:\n",
    "        task_val[i] = task_val[i][:len(input_ids_val[\"Advertise\"]) * 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "def reshapeArray(input_ids:dict, input_types:dict, input_masks:dict, y:dict, task:dict):\n",
    "    index = [int(len(input_ids[\"Phish\"])/BATCH_SIZE) * BATCH_SIZE, int(len(input_ids[\"Multi\"])/BATCH_SIZE) * BATCH_SIZE, int(len(input_ids[\"Advertise\"])/BATCH_SIZE) * BATCH_SIZE]\n",
    "    input_ids[\"Phish\"] = input_ids[\"Phish\"][:index[0]]\n",
    "    input_ids[\"Multi\"] = input_ids[\"Multi\"][:index[1]]\n",
    "    input_ids[\"Advertise\"] = input_ids[\"Advertise\"][:index[2]]\n",
    "\n",
    "    input_types[\"Phish\"] = input_types[\"Phish\"][:index[0]]\n",
    "    input_types[\"Multi\"] = input_types[\"Multi\"][:index[1]]\n",
    "    input_types[\"Advertise\"] = input_types[\"Advertise\"][:index[2]]\n",
    "\n",
    "    input_masks[\"Phish\"] = input_masks[\"Phish\"][:index[0]]\n",
    "    input_masks[\"Multi\"] = input_masks[\"Multi\"][:index[1]]\n",
    "    input_masks[\"Advertise\"] = input_masks[\"Advertise\"][:index[2]]\n",
    "\n",
    "    y[\"Phish\"] = y[\"Phish\"][:index[0]]\n",
    "    y[\"Multi\"] = y[\"Multi\"][:index[1]]\n",
    "    y[\"Advertise\"] = y[\"Advertise\"][:index[2]]\n",
    "\n",
    "    task[\"Phish\"] = task[\"Phish\"][:index[0]]\n",
    "    task[\"Multi\"] = task[\"Multi\"][:index[1]]\n",
    "    task[\"Advertise\"] = task[\"Advertise\"][:index[2]]\n",
    "\n",
    "    return input_ids, input_types, input_masks, y, task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "input_ids_train, input_types_train, input_masks_train, y_train, task_train = reshapeArray(input_ids_train, input_types_train, input_masks_train, y_train, task_train)\n",
    "input_ids_val, input_types_val, input_masks_val, y_val, task_val = reshapeArray(input_ids_val, input_types_val, input_masks_val, y_val, task_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112896, 200)\n",
      "(37632, 1)\n",
      "(112896, 200)\n"
     ]
    }
   ],
   "source": [
    "print(input_ids_train[\"Multi\"].shape)\n",
    "print(y_train[\"Advertise\"].shape)\n",
    "print(input_ids_train[\"Phish\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1764\n",
      "441\n"
     ]
    }
   ],
   "source": [
    "BATCH_NUM_TASK_TRAIN = int(input_ids_train[\"Multi\"].shape[0]/BATCH_SIZE)\n",
    "BATCH_NUM_TASK_VAL = int(input_ids_val[\"Multi\"].shape[0]/BATCH_SIZE)\n",
    "BATCH_NUM_ADV_TRAIN = int(input_ids_train[\"Advertise\"].shape[0]/BATCH_SIZE)\n",
    "BATCH_NUM_ADV_VAL = int(input_ids_val[\"Advertise\"].shape[0]/BATCH_SIZE)\n",
    "print(BATCH_NUM_TASK_TRAIN)\n",
    "print(BATCH_NUM_TASK_VAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for i in input_ids_train:\n",
    "    if i == \"Advertise\":\n",
    "        input_ids_train[i] = input_ids_train[i].reshape((BATCH_NUM_ADV_TRAIN, BATCH_SIZE, 200))\n",
    "    else:\n",
    "        input_ids_train[i] = input_ids_train[i].reshape((BATCH_NUM_TASK_TRAIN, BATCH_SIZE, 200))\n",
    "for i in input_types_train:\n",
    "    if i == \"Advertise\":\n",
    "        input_types_train[i] = input_types_train[i].reshape((BATCH_NUM_ADV_TRAIN, BATCH_SIZE, 200))\n",
    "    else:\n",
    "        input_types_train[i] = input_types_train[i].reshape((BATCH_NUM_TASK_TRAIN, BATCH_SIZE, 200))\n",
    "for i in input_masks_train:\n",
    "    if i == \"Advertise\":\n",
    "        input_masks_train[i] = input_masks_train[i].reshape((BATCH_NUM_ADV_TRAIN, BATCH_SIZE, 200))\n",
    "    else:\n",
    "        input_masks_train[i] = input_masks_train[i].reshape((BATCH_NUM_TASK_TRAIN, BATCH_SIZE, 200))\n",
    "for i in y_train:\n",
    "    if i == \"Advertise\":\n",
    "        y_train[i] = y_train[i].reshape((BATCH_NUM_ADV_TRAIN, BATCH_SIZE, 1))\n",
    "    else:\n",
    "        y_train[i] = y_train[i].reshape((BATCH_NUM_TASK_TRAIN, BATCH_SIZE, 1))\n",
    "for i in task_train:\n",
    "    if i == \"Advertise\":\n",
    "        task_train[i] = task_train[i].reshape((BATCH_NUM_ADV_TRAIN, BATCH_SIZE, 1))\n",
    "    else:\n",
    "        task_train[i] = task_train[i].reshape((BATCH_NUM_TASK_TRAIN, BATCH_SIZE, 1))\n",
    "for i in input_ids_val:\n",
    "    if i == \"Advertise\":\n",
    "        input_ids_val[i] = input_ids_val[i].reshape((BATCH_NUM_ADV_VAL, BATCH_SIZE, 200))\n",
    "    else:\n",
    "        input_ids_val[i] = input_ids_val[i].reshape((BATCH_NUM_TASK_VAL, BATCH_SIZE, 200))\n",
    "for i in input_types_val:\n",
    "    if i == \"Advertise\":\n",
    "        input_types_val[i] = input_types_val[i].reshape((BATCH_NUM_ADV_VAL, BATCH_SIZE, 200))\n",
    "    else:\n",
    "        input_types_val[i] = input_types_val[i].reshape((BATCH_NUM_TASK_VAL, BATCH_SIZE, 200))\n",
    "for i in input_masks_val:\n",
    "    if i == \"Advertise\":\n",
    "        input_masks_val[i] = input_masks_val[i].reshape((BATCH_NUM_ADV_VAL, BATCH_SIZE, 200))\n",
    "    else:\n",
    "        input_masks_val[i] = input_masks_val[i].reshape((BATCH_NUM_TASK_VAL, BATCH_SIZE, 200))\n",
    "for i in y_val:\n",
    "    if i == \"Advertise\":\n",
    "        y_val[i] = y_val[i].reshape((BATCH_NUM_ADV_VAL, BATCH_SIZE, 1))\n",
    "    else:\n",
    "        y_val[i] = y_val[i].reshape((BATCH_NUM_TASK_VAL, BATCH_SIZE, 1))\n",
    "for i in task_val:\n",
    "    if i == \"Advertise\":\n",
    "        task_val[i] = task_val[i].reshape((BATCH_NUM_ADV_VAL, BATCH_SIZE, 1))\n",
    "    else:\n",
    "        task_val[i] = task_val[i].reshape((BATCH_NUM_TASK_VAL, BATCH_SIZE, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1764, 64, 200)\n",
      "(1764, 64, 200)\n",
      "(588, 64, 200)\n"
     ]
    }
   ],
   "source": [
    "for i in input_ids_train:\n",
    "    print(input_ids_train[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "input_ids_mtl_train = np.concatenate(tuple([input_ids_train[i] for i in input_ids_train]),axis=0)\n",
    "input_types_mtl_train = np.concatenate(tuple([input_types_train[i] for i in input_types_train]), axis=0)\n",
    "input_masks_mtl_train = np.concatenate(tuple([input_masks_train[i] for i in input_masks_train]), axis=0)\n",
    "y_mtl_train = np.concatenate(tuple([y_train[i] for i in y_train]), axis=0)\n",
    "task_mtl_train = np.concatenate(tuple([task_train[i] for i in task_train]), axis=0)\n",
    "input_ids_mtl_val = np.concatenate(tuple([input_ids_val[i] for i in input_ids_val]), axis=0)\n",
    "input_types_mtl_val = np.concatenate(tuple([input_types_val[i] for i in input_types_val]), axis=0)\n",
    "input_masks_mtl_val = np.concatenate(tuple([input_masks_val[i] for i in input_masks_val]), axis=0)\n",
    "y_mtl_val = np.concatenate(tuple([y_val[i] for i in y_val]), axis=0)\n",
    "task_mtl_val = np.concatenate(tuple([task_val[i] for i in task_val]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1514 2197 1192 2495   69 2323 1519  502 4093 3516]\n",
      "[803 666 428  62 525 489 687 329 173 186]\n",
      "(4116, 64, 200)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2024)  # Fix the seed\n",
    "random_order = np.arange(len(input_ids_mtl_train))\n",
    "np.random.shuffle(random_order)\n",
    "print(random_order[:10])\n",
    "input_ids_mtl_train = input_ids_mtl_train[random_order]\n",
    "input_types_mtl_train = input_types_mtl_train[random_order]\n",
    "input_masks_mtl_train = input_masks_mtl_train[random_order]\n",
    "y_mtl_train = y_mtl_train[random_order]\n",
    "task_mtl_train = task_mtl_train[random_order]\n",
    "\n",
    "random_order = np.arange(len(input_ids_mtl_val))\n",
    "np.random.shuffle(random_order)\n",
    "print(random_order[:10])\n",
    "input_ids_mtl_val = input_ids_mtl_val[random_order]\n",
    "input_types_mtl_val = input_types_mtl_val[random_order]\n",
    "input_masks_mtl_val = input_masks_mtl_val[random_order]\n",
    "y_mtl_val = y_mtl_val[random_order]\n",
    "task_mtl_val = task_mtl_val[random_order]\n",
    "\n",
    "print(input_ids_mtl_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "input_ids_mtl_train = input_ids_mtl_train.reshape(BATCH_NUM_TASK_TRAIN * BATCH_SIZE * 2 + BATCH_NUM_ADV_TRAIN * BATCH_SIZE, 200)\n",
    "input_types_mtl_train = input_types_mtl_train.reshape(BATCH_NUM_TASK_TRAIN * BATCH_SIZE * 2 + BATCH_NUM_ADV_TRAIN * BATCH_SIZE, 200)\n",
    "input_masks_mtl_train = input_masks_mtl_train.reshape(BATCH_NUM_TASK_TRAIN * BATCH_SIZE * 2 + BATCH_NUM_ADV_TRAIN * BATCH_SIZE, 200)\n",
    "y_mtl_train = y_mtl_train.reshape(BATCH_NUM_TASK_TRAIN * BATCH_SIZE * 2 + BATCH_NUM_ADV_TRAIN * BATCH_SIZE, 1)\n",
    "task_mtl_train = task_mtl_train.reshape(BATCH_NUM_TASK_TRAIN * BATCH_SIZE * 2 + BATCH_NUM_ADV_TRAIN * BATCH_SIZE, 1)\n",
    "input_ids_mtl_val = input_ids_mtl_val.reshape(BATCH_NUM_TASK_VAL * BATCH_SIZE * 2 + BATCH_NUM_ADV_VAL * BATCH_SIZE, 200)\n",
    "input_types_mtl_val = input_types_mtl_val.reshape(BATCH_NUM_TASK_VAL * BATCH_SIZE * 2 + BATCH_NUM_ADV_VAL * BATCH_SIZE, 200)\n",
    "input_masks_mtl_val = input_masks_mtl_val.reshape(BATCH_NUM_TASK_VAL * BATCH_SIZE * 2 + BATCH_NUM_ADV_VAL * BATCH_SIZE, 200)\n",
    "y_mtl_val = y_mtl_val.reshape(BATCH_NUM_TASK_VAL * BATCH_SIZE * 2 + BATCH_NUM_ADV_VAL * BATCH_SIZE, 1)\n",
    "task_mtl_val = task_mtl_val.reshape(BATCH_NUM_TASK_VAL * BATCH_SIZE * 2 + BATCH_NUM_ADV_VAL * BATCH_SIZE, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(263424, 200)\n",
      "(65856, 1)\n"
     ]
    }
   ],
   "source": [
    "print(input_ids_mtl_train.shape)\n",
    "print(task_mtl_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_data = TensorDataset(torch.tensor(input_ids_mtl_train).to(DEVICE),\n",
    "                           torch.tensor(input_types_mtl_train).to(DEVICE),\n",
    "                           torch.tensor(input_masks_mtl_train).to(DEVICE),\n",
    "                           torch.tensor(y_mtl_train).to(DEVICE),\n",
    "                           torch.tensor(task_mtl_train).to(DEVICE))\n",
    "train_sampler = SequentialSampler(train_data)\n",
    "train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "val_data = TensorDataset(torch.tensor(input_ids_mtl_val).to(DEVICE),\n",
    "                         torch.tensor(input_types_mtl_val).to(DEVICE),\n",
    "                         torch.tensor(input_masks_mtl_val).to(DEVICE),\n",
    "                         torch.tensor(y_mtl_val).to(DEVICE),\n",
    "                         torch.tensor(task_mtl_val).to(DEVICE))\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_loader = DataLoader(val_data, sampler=val_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_name_or_path\": \"./bert_model/bert_config/\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.36.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 5000\n",
      "}\n",
      "\n",
      "BertForMaskedLM(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(5000, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (cls): BertOnlyMLMHead(\n",
      "    (predictions): BertLMPredictionHead(\n",
      "      (transform): BertPredictionHeadTransform(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (transform_act_fn): GELUActivation()\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (decoder): Linear(in_features=768, out_features=5000, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "config_kwargs = {\n",
    "    \"cache_dir\": None,\n",
    "    \"revision\": 'main',\n",
    "    \"use_auth_token\": None,\n",
    "    \"hidden_dropout_prob\": 0.1,\n",
    "    \"vocab_size\": 5000,\n",
    "}\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"path_to_the_config\", **config_kwargs)\n",
    "print(config)\n",
    "\n",
    "bert_model = AutoModelForMaskedLM.from_config(\n",
    "    config=config,\n",
    ")\n",
    "bert_model.resize_token_embeddings(config_kwargs[\"vocab_size\"])\n",
    "print(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_dict = torch.load(\"path_to_the_model\", map_location='cpu')\n",
    "bert_model.load_state_dict(bert_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class BertForSequenceClassification(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(BertForSequenceClassification, self).__init__()\n",
    "        self.bert = bert\n",
    "        for name, param in self.bert.named_parameters():\n",
    "            param.requires_grad = True\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.classifier_0 = nn.Linear(768, 2)\n",
    "        self.classifier_1 = nn.Linear(768, 5)\n",
    "        self.classifier_2 = nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        context = x[0]\n",
    "        types = x[1]\n",
    "        mask = x[2]\n",
    "        task = x[3]\n",
    "        outputs = self.bert(context, attention_mask=mask, token_type_ids=types, output_hidden_states=True)\n",
    "        hidden_states = outputs.hidden_states[-1][:,0,:]\n",
    "        out = self.dropout(hidden_states)\n",
    "        if task[0][0] == 0 and torch.all(task == task[0]):\n",
    "            out = self.classifier_0(out)\n",
    "        elif task[0][0] == 1 and torch.all(task == task[0]):\n",
    "            out = self.classifier_1(out)\n",
    "        elif task[0][0] == 2 and torch.all(task == task[0]):\n",
    "            out = self.classifier_2(out)\n",
    "        else:\n",
    "            print(\"something wrong with the data\")\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertForMaskedLM(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(5000, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): Sequential()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier_0): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (classifier_1): Linear(in_features=768, out_features=5, bias=True)\n",
       "  (classifier_2): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification(bert_model)\n",
    "model.bert.cls = nn.Sequential()\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (x1, x2, x3, y, task) in enumerate(train_loader):\n",
    "        x1, x2, x3, y, task = x1.to(device), x2.to(device), x3.to(device), y.to(device), task.to(device)\n",
    "        assert torch.all(task == task[0])\n",
    "        y_pred = model([x1, x2, x3, task])\n",
    "        model.zero_grad()\n",
    "\n",
    "        loss = F.cross_entropy(y_pred, y.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if task[0][0] == 0 and torch.all(task == task[0]):\n",
    "            task_name = \"Phish\"\n",
    "        elif task[0][0] == 1 and torch.all(task == task[0]):\n",
    "            task_name = \"Multi\"\n",
    "        elif task[0][0] == 2 and torch.all(task == task[0]):\n",
    "            task_name = \"Advertise\"\n",
    "        else:\n",
    "            task_name = \"None\"\n",
    "\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.2f}%)]\\t Loss: {:.6f} Task: {}'.format(epoch, (batch_idx + 1) * len(x1),\n",
    "                                                                            len(train_loader.dataset),\n",
    "                                                                            100. * batch_idx / len(train_loader),\n",
    "                                                                            loss.item(), task_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def validation(model, device, test_loader):\n",
    "    \"\"\"\n",
    "    Perform model validation on the test data.\n",
    "\n",
    "    :param model: The model to be validated.\n",
    "    :param device: The device to run validation on (e.g., CPU or GPU).\n",
    "    :param test_loader: The data loader for test data.\n",
    "    :return: A tuple containing accuracy, precision, recall, and F1 score.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss = [0.0, 0.0, 0.0]\n",
    "    test_len = [0, 0, 0]\n",
    "    task_name = [\"Phish\", \"Multi\", \"Advertise\"]\n",
    "    y_true = {\"Phish\":[], \"Multi\":[], \"Advertise\":[]}\n",
    "    y_pred = {\"Phish\":[], \"Multi\":[], \"Advertise\":[]}\n",
    "\n",
    "    for batch_idx, (x1, x2, x3, y, task) in enumerate(test_loader):\n",
    "        x1, x2, x3, y, task = x1.to(device), x2.to(device), x3.to(device), y.to(device), task.to(device)\n",
    "        assert torch.all(task == task[0])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_ = model([x1, x2, x3, task])\n",
    "\n",
    "        if task[0][0] == 0 and torch.all(task == task[0]):\n",
    "            test_loss[0] += F.cross_entropy(y_, y.squeeze()).item()\n",
    "            test_len[0] += 1\n",
    "        if task[0][0] == 1 and torch.all(task == task[0]):\n",
    "            test_loss[1] += F.cross_entropy(y_, y.squeeze()).item()\n",
    "            test_len[1] += 1\n",
    "        if task[0][0] == 2 and torch.all(task == task[0]):\n",
    "            test_loss[2] += F.cross_entropy(y_, y.squeeze()).item()\n",
    "            test_len[2] += 1\n",
    "\n",
    "        pred = y_.max(-1, keepdim=True)[1]  # .max(): 2 outputs, representing the maximum value and its index\n",
    "\n",
    "        if task[0][0] == 0 and torch.all(task == task[0]):\n",
    "            y_true[\"Phish\"].extend(y.cpu().numpy())\n",
    "            y_pred[\"Phish\"].extend(pred.cpu().numpy())\n",
    "        elif task[0][0] == 1 and torch.all(task == task[0]):\n",
    "            y_true[\"Multi\"].extend(y.cpu().numpy())\n",
    "            y_pred[\"Multi\"].extend(pred.cpu().numpy())\n",
    "        elif task[0][0] == 2 and torch.all(task == task[0]):\n",
    "            y_true[\"Advertise\"].extend(y.cpu().numpy())\n",
    "            y_pred[\"Advertise\"].extend(pred.cpu().numpy())\n",
    "\n",
    "    test_loss[0] /= test_len[0]\n",
    "    test_loss[1] /= test_len[1]\n",
    "    test_loss[2] /= test_len[2]\n",
    "\n",
    "    accuracy = {\"Phish\":0.0, \"Multi\":0.0, \"Advertise\":0.0}\n",
    "    precision = {\"Phish\":0.0, \"Multi\":0.0, \"Advertise\":0.0}\n",
    "    recall = {\"Phish\":0.0, \"Multi\":0.0, \"Advertise\":0.0}\n",
    "    f1 = {\"Phish\":0.0, \"Multi\":0.0, \"Advertise\":0.0}\n",
    "\n",
    "    for i in accuracy:\n",
    "        accuracy[i] = accuracy_score(y_true[i], y_pred[i])\n",
    "    for i in precision:\n",
    "        precision[i] = precision_score(y_true[i], y_pred[i], average='macro')\n",
    "    for i in recall:\n",
    "        recall[i] = recall_score(y_true[i], y_pred[i], average='macro')\n",
    "    for i in f1:\n",
    "        f1[i] = f1_score(y_true[i], y_pred[i], average='macro')\n",
    "\n",
    "    print('Test set: Task: {} Average loss: {:.4f}, Accuracy: {:.2f}%, Precision: {:.2f}%, Recall: {:.2f}%, F1: {:.2f}%'.format(\n",
    "        task_name[0], test_loss[0], accuracy[\"Phish\"] * 100, precision[\"Phish\"] * 100, recall[\"Phish\"] * 100, f1[\"Phish\"] * 100))\n",
    "\n",
    "    print('Test set: Task: {} Average loss: {:.4f}, Accuracy: {:.2f}%, Precision: {:.2f}%, Recall: {:.2f}%, F1: {:.2f}%'.format(\n",
    "        task_name[1], test_loss[1], accuracy[\"Multi\"] * 100, precision[\"Multi\"] * 100, recall[\"Multi\"] * 100, f1[\"Multi\"] * 100))\n",
    "\n",
    "    print('Test set: Task: {} Average loss: {:.4f}, Accuracy: {:.2f}%, Precision: {:.2f}%, Recall: {:.2f}%, F1: {:.2f}%'.format(\n",
    "        task_name[2], test_loss[2], accuracy[\"Advertise\"] * 100, precision[\"Advertise\"] * 100, recall[\"Advertise\"] * 100, f1[\"Phish\"] * 100))\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [6400/263424 (2.41%)]\t Loss: 0.326429 Task: Phish\n",
      "Train Epoch: 1 [12800/263424 (4.83%)]\t Loss: 0.447186 Task: Phish\n",
      "Train Epoch: 1 [19200/263424 (7.26%)]\t Loss: 1.359781 Task: Multi\n",
      "Train Epoch: 1 [25600/263424 (9.69%)]\t Loss: 1.358064 Task: Multi\n",
      "Train Epoch: 1 [32000/263424 (12.12%)]\t Loss: 1.285958 Task: Multi\n",
      "Train Epoch: 1 [38400/263424 (14.55%)]\t Loss: 1.238685 Task: Multi\n",
      "Train Epoch: 1 [44800/263424 (16.98%)]\t Loss: 1.446838 Task: Multi\n",
      "Train Epoch: 1 [51200/263424 (19.41%)]\t Loss: 1.401391 Task: Multi\n",
      "Train Epoch: 1 [57600/263424 (21.84%)]\t Loss: 1.369311 Task: Multi\n",
      "Train Epoch: 1 [64000/263424 (24.27%)]\t Loss: 0.028417 Task: Advertise\n",
      "Train Epoch: 1 [70400/263424 (26.70%)]\t Loss: 0.226673 Task: Phish\n",
      "Train Epoch: 1 [76800/263424 (29.13%)]\t Loss: 1.083943 Task: Multi\n",
      "Train Epoch: 1 [83200/263424 (31.56%)]\t Loss: 0.253826 Task: Phish\n",
      "Train Epoch: 1 [89600/263424 (33.99%)]\t Loss: 0.151376 Task: Phish\n",
      "Train Epoch: 1 [96000/263424 (36.42%)]\t Loss: 1.123507 Task: Multi\n",
      "Train Epoch: 1 [102400/263424 (38.85%)]\t Loss: 0.145810 Task: Phish\n",
      "Train Epoch: 1 [108800/263424 (41.28%)]\t Loss: 1.170733 Task: Multi\n",
      "Train Epoch: 1 [115200/263424 (43.71%)]\t Loss: 0.154188 Task: Advertise\n",
      "Train Epoch: 1 [121600/263424 (46.14%)]\t Loss: 0.215620 Task: Phish\n",
      "Train Epoch: 1 [128000/263424 (48.57%)]\t Loss: 0.937447 Task: Multi\n",
      "Train Epoch: 1 [134400/263424 (51.00%)]\t Loss: 0.146826 Task: Phish\n",
      "Train Epoch: 1 [140800/263424 (53.43%)]\t Loss: 0.304179 Task: Phish\n",
      "Train Epoch: 1 [147200/263424 (55.86%)]\t Loss: 0.844863 Task: Multi\n",
      "Train Epoch: 1 [153600/263424 (58.28%)]\t Loss: 1.236684 Task: Multi\n",
      "Train Epoch: 1 [160000/263424 (60.71%)]\t Loss: 1.475655 Task: Multi\n",
      "Train Epoch: 1 [166400/263424 (63.14%)]\t Loss: 0.997475 Task: Multi\n",
      "Train Epoch: 1 [172800/263424 (65.57%)]\t Loss: 1.137540 Task: Multi\n",
      "Train Epoch: 1 [179200/263424 (68.00%)]\t Loss: 0.030094 Task: Advertise\n",
      "Train Epoch: 1 [185600/263424 (70.43%)]\t Loss: 1.004129 Task: Multi\n",
      "Train Epoch: 1 [192000/263424 (72.86%)]\t Loss: 0.167659 Task: Phish\n",
      "Train Epoch: 1 [198400/263424 (75.29%)]\t Loss: 0.139440 Task: Phish\n",
      "Train Epoch: 1 [204800/263424 (77.72%)]\t Loss: 0.030650 Task: Advertise\n",
      "Train Epoch: 1 [211200/263424 (80.15%)]\t Loss: 0.359840 Task: Advertise\n",
      "Train Epoch: 1 [217600/263424 (82.58%)]\t Loss: 0.172526 Task: Phish\n",
      "Train Epoch: 1 [224000/263424 (85.01%)]\t Loss: 1.016537 Task: Multi\n",
      "Train Epoch: 1 [230400/263424 (87.44%)]\t Loss: 1.060964 Task: Multi\n",
      "Train Epoch: 1 [236800/263424 (89.87%)]\t Loss: 0.009278 Task: Advertise\n",
      "Train Epoch: 1 [243200/263424 (92.30%)]\t Loss: 0.221697 Task: Phish\n",
      "Train Epoch: 1 [249600/263424 (94.73%)]\t Loss: 0.087572 Task: Phish\n",
      "Train Epoch: 1 [256000/263424 (97.16%)]\t Loss: 0.904029 Task: Multi\n",
      "Train Epoch: 1 [262400/263424 (99.59%)]\t Loss: 0.063466 Task: Advertise\n",
      "Test set: Task: Phish Average loss: 0.1811, Accuracy: 92.98%, Precision: 93.03%, Recall: 92.97%, F1: 92.97%\n",
      "Test set: Task: Multi Average loss: 1.0007, Accuracy: 63.02%, Precision: 60.04%, Recall: 58.17%, F1: 58.11%\n",
      "Test set: Task: Advertise Average loss: 0.0355, Accuracy: 98.94%, Precision: 98.94%, Recall: 98.94%, F1: 92.97%\n",
      "Phish: acc is: 0.9298, best acc is 0.9298\n",
      "\n",
      "Multi: acc is: 0.6302, best acc is 0.6302\n",
      "\n",
      "Advertise: acc is: 0.9894, best acc is 0.9894\n",
      "\n",
      "Train Epoch: 2 [6400/263424 (2.41%)]\t Loss: 0.091470 Task: Phish\n",
      "Train Epoch: 2 [12800/263424 (4.83%)]\t Loss: 0.133708 Task: Phish\n",
      "Train Epoch: 2 [19200/263424 (7.26%)]\t Loss: 1.010267 Task: Multi\n",
      "Train Epoch: 2 [25600/263424 (9.69%)]\t Loss: 1.152481 Task: Multi\n",
      "Train Epoch: 2 [32000/263424 (12.12%)]\t Loss: 0.969876 Task: Multi\n",
      "Train Epoch: 2 [38400/263424 (14.55%)]\t Loss: 0.819973 Task: Multi\n",
      "Train Epoch: 2 [44800/263424 (16.98%)]\t Loss: 1.034280 Task: Multi\n",
      "Train Epoch: 2 [51200/263424 (19.41%)]\t Loss: 0.950229 Task: Multi\n",
      "Train Epoch: 2 [57600/263424 (21.84%)]\t Loss: 0.869181 Task: Multi\n",
      "Train Epoch: 2 [64000/263424 (24.27%)]\t Loss: 0.009616 Task: Advertise\n",
      "Train Epoch: 2 [70400/263424 (26.70%)]\t Loss: 0.114465 Task: Phish\n",
      "Train Epoch: 2 [76800/263424 (29.13%)]\t Loss: 1.056137 Task: Multi\n",
      "Train Epoch: 2 [83200/263424 (31.56%)]\t Loss: 0.150426 Task: Phish\n",
      "Train Epoch: 2 [89600/263424 (33.99%)]\t Loss: 0.139709 Task: Phish\n",
      "Train Epoch: 2 [96000/263424 (36.42%)]\t Loss: 0.991088 Task: Multi\n",
      "Train Epoch: 2 [102400/263424 (38.85%)]\t Loss: 0.100400 Task: Phish\n",
      "Train Epoch: 2 [108800/263424 (41.28%)]\t Loss: 1.118285 Task: Multi\n",
      "Train Epoch: 2 [115200/263424 (43.71%)]\t Loss: 0.034091 Task: Advertise\n",
      "Train Epoch: 2 [121600/263424 (46.14%)]\t Loss: 0.052588 Task: Phish\n",
      "Train Epoch: 2 [128000/263424 (48.57%)]\t Loss: 0.773772 Task: Multi\n",
      "Train Epoch: 2 [134400/263424 (51.00%)]\t Loss: 0.060994 Task: Phish\n",
      "Train Epoch: 2 [140800/263424 (53.43%)]\t Loss: 0.141809 Task: Phish\n",
      "Train Epoch: 2 [147200/263424 (55.86%)]\t Loss: 0.698443 Task: Multi\n",
      "Train Epoch: 2 [153600/263424 (58.28%)]\t Loss: 0.928639 Task: Multi\n",
      "Train Epoch: 2 [160000/263424 (60.71%)]\t Loss: 1.240905 Task: Multi\n",
      "Train Epoch: 2 [166400/263424 (63.14%)]\t Loss: 0.842551 Task: Multi\n",
      "Train Epoch: 2 [172800/263424 (65.57%)]\t Loss: 1.017469 Task: Multi\n",
      "Train Epoch: 2 [179200/263424 (68.00%)]\t Loss: 0.050694 Task: Advertise\n",
      "Train Epoch: 2 [185600/263424 (70.43%)]\t Loss: 0.980369 Task: Multi\n",
      "Train Epoch: 2 [192000/263424 (72.86%)]\t Loss: 0.135034 Task: Phish\n",
      "Train Epoch: 2 [198400/263424 (75.29%)]\t Loss: 0.122935 Task: Phish\n",
      "Train Epoch: 2 [204800/263424 (77.72%)]\t Loss: 0.002093 Task: Advertise\n",
      "Train Epoch: 2 [211200/263424 (80.15%)]\t Loss: 0.244405 Task: Advertise\n",
      "Train Epoch: 2 [217600/263424 (82.58%)]\t Loss: 0.124738 Task: Phish\n",
      "Train Epoch: 2 [224000/263424 (85.01%)]\t Loss: 1.000372 Task: Multi\n",
      "Train Epoch: 2 [230400/263424 (87.44%)]\t Loss: 0.980961 Task: Multi\n",
      "Train Epoch: 2 [236800/263424 (89.87%)]\t Loss: 0.010655 Task: Advertise\n",
      "Train Epoch: 2 [243200/263424 (92.30%)]\t Loss: 0.231962 Task: Phish\n",
      "Train Epoch: 2 [249600/263424 (94.73%)]\t Loss: 0.069024 Task: Phish\n",
      "Train Epoch: 2 [256000/263424 (97.16%)]\t Loss: 0.826453 Task: Multi\n",
      "Train Epoch: 2 [262400/263424 (99.59%)]\t Loss: 0.011681 Task: Advertise\n",
      "Test set: Task: Phish Average loss: 0.1587, Accuracy: 94.19%, Precision: 94.23%, Recall: 94.17%, F1: 94.18%\n",
      "Test set: Task: Multi Average loss: 0.9022, Accuracy: 67.59%, Precision: 66.69%, Recall: 63.55%, F1: 64.24%\n",
      "Test set: Task: Advertise Average loss: 0.0305, Accuracy: 99.12%, Precision: 99.12%, Recall: 99.12%, F1: 94.18%\n",
      "Phish: acc is: 0.9419, best acc is 0.9419\n",
      "\n",
      "Multi: acc is: 0.6759, best acc is 0.6759\n",
      "\n",
      "Advertise: acc is: 0.9912, best acc is 0.9912\n",
      "\n",
      "Train Epoch: 3 [6400/263424 (2.41%)]\t Loss: 0.075044 Task: Phish\n",
      "Train Epoch: 3 [12800/263424 (4.83%)]\t Loss: 0.103096 Task: Phish\n",
      "Train Epoch: 3 [19200/263424 (7.26%)]\t Loss: 0.884278 Task: Multi\n",
      "Train Epoch: 3 [25600/263424 (9.69%)]\t Loss: 1.041644 Task: Multi\n",
      "Train Epoch: 3 [32000/263424 (12.12%)]\t Loss: 0.935705 Task: Multi\n",
      "Train Epoch: 3 [38400/263424 (14.55%)]\t Loss: 0.726439 Task: Multi\n",
      "Train Epoch: 3 [44800/263424 (16.98%)]\t Loss: 0.974254 Task: Multi\n",
      "Train Epoch: 3 [51200/263424 (19.41%)]\t Loss: 1.006402 Task: Multi\n",
      "Train Epoch: 3 [57600/263424 (21.84%)]\t Loss: 0.822202 Task: Multi\n",
      "Train Epoch: 3 [64000/263424 (24.27%)]\t Loss: 0.002400 Task: Advertise\n",
      "Train Epoch: 3 [70400/263424 (26.70%)]\t Loss: 0.130268 Task: Phish\n",
      "Train Epoch: 3 [76800/263424 (29.13%)]\t Loss: 1.082552 Task: Multi\n",
      "Train Epoch: 3 [83200/263424 (31.56%)]\t Loss: 0.106021 Task: Phish\n",
      "Train Epoch: 3 [89600/263424 (33.99%)]\t Loss: 0.110420 Task: Phish\n",
      "Train Epoch: 3 [96000/263424 (36.42%)]\t Loss: 0.968662 Task: Multi\n",
      "Train Epoch: 3 [102400/263424 (38.85%)]\t Loss: 0.090375 Task: Phish\n",
      "Train Epoch: 3 [108800/263424 (41.28%)]\t Loss: 0.881777 Task: Multi\n",
      "Train Epoch: 3 [115200/263424 (43.71%)]\t Loss: 0.021802 Task: Advertise\n",
      "Train Epoch: 3 [121600/263424 (46.14%)]\t Loss: 0.086045 Task: Phish\n",
      "Train Epoch: 3 [128000/263424 (48.57%)]\t Loss: 0.741875 Task: Multi\n",
      "Train Epoch: 3 [134400/263424 (51.00%)]\t Loss: 0.061552 Task: Phish\n",
      "Train Epoch: 3 [140800/263424 (53.43%)]\t Loss: 0.133897 Task: Phish\n",
      "Train Epoch: 3 [147200/263424 (55.86%)]\t Loss: 0.679050 Task: Multi\n",
      "Train Epoch: 3 [153600/263424 (58.28%)]\t Loss: 0.851654 Task: Multi\n",
      "Train Epoch: 3 [160000/263424 (60.71%)]\t Loss: 1.080847 Task: Multi\n",
      "Train Epoch: 3 [166400/263424 (63.14%)]\t Loss: 0.777486 Task: Multi\n",
      "Train Epoch: 3 [172800/263424 (65.57%)]\t Loss: 0.867411 Task: Multi\n",
      "Train Epoch: 3 [179200/263424 (68.00%)]\t Loss: 0.002253 Task: Advertise\n",
      "Train Epoch: 3 [185600/263424 (70.43%)]\t Loss: 0.937674 Task: Multi\n",
      "Train Epoch: 3 [192000/263424 (72.86%)]\t Loss: 0.094685 Task: Phish\n",
      "Train Epoch: 3 [198400/263424 (75.29%)]\t Loss: 0.082903 Task: Phish\n",
      "Train Epoch: 3 [204800/263424 (77.72%)]\t Loss: 0.002945 Task: Advertise\n",
      "Train Epoch: 3 [211200/263424 (80.15%)]\t Loss: 0.268316 Task: Advertise\n",
      "Train Epoch: 3 [217600/263424 (82.58%)]\t Loss: 0.110076 Task: Phish\n",
      "Train Epoch: 3 [224000/263424 (85.01%)]\t Loss: 0.978008 Task: Multi\n",
      "Train Epoch: 3 [230400/263424 (87.44%)]\t Loss: 0.838572 Task: Multi\n",
      "Train Epoch: 3 [236800/263424 (89.87%)]\t Loss: 0.002830 Task: Advertise\n",
      "Train Epoch: 3 [243200/263424 (92.30%)]\t Loss: 0.261601 Task: Phish\n",
      "Train Epoch: 3 [249600/263424 (94.73%)]\t Loss: 0.067465 Task: Phish\n",
      "Train Epoch: 3 [256000/263424 (97.16%)]\t Loss: 0.706656 Task: Multi\n",
      "Train Epoch: 3 [262400/263424 (99.59%)]\t Loss: 0.005041 Task: Advertise\n",
      "Test set: Task: Phish Average loss: 0.1387, Accuracy: 94.84%, Precision: 94.87%, Recall: 94.85%, F1: 94.84%\n",
      "Test set: Task: Multi Average loss: 0.8527, Accuracy: 69.64%, Precision: 69.28%, Recall: 66.13%, F1: 66.93%\n",
      "Test set: Task: Advertise Average loss: 0.0228, Accuracy: 99.42%, Precision: 99.42%, Recall: 99.42%, F1: 94.84%\n",
      "Phish: acc is: 0.9484, best acc is 0.9484\n",
      "\n",
      "Multi: acc is: 0.6964, best acc is 0.6964\n",
      "\n",
      "Advertise: acc is: 0.9942, best acc is 0.9942\n",
      "\n",
      "Train Epoch: 4 [6400/263424 (2.41%)]\t Loss: 0.070946 Task: Phish\n",
      "Train Epoch: 4 [12800/263424 (4.83%)]\t Loss: 0.103971 Task: Phish\n",
      "Train Epoch: 4 [19200/263424 (7.26%)]\t Loss: 0.886579 Task: Multi\n",
      "Train Epoch: 4 [25600/263424 (9.69%)]\t Loss: 0.972226 Task: Multi\n",
      "Train Epoch: 4 [32000/263424 (12.12%)]\t Loss: 0.835772 Task: Multi\n",
      "Train Epoch: 4 [38400/263424 (14.55%)]\t Loss: 0.759178 Task: Multi\n",
      "Train Epoch: 4 [44800/263424 (16.98%)]\t Loss: 0.934064 Task: Multi\n",
      "Train Epoch: 4 [51200/263424 (19.41%)]\t Loss: 0.848903 Task: Multi\n",
      "Train Epoch: 4 [57600/263424 (21.84%)]\t Loss: 0.675999 Task: Multi\n",
      "Train Epoch: 4 [64000/263424 (24.27%)]\t Loss: 0.002848 Task: Advertise\n",
      "Train Epoch: 4 [70400/263424 (26.70%)]\t Loss: 0.077378 Task: Phish\n",
      "Train Epoch: 4 [76800/263424 (29.13%)]\t Loss: 1.036338 Task: Multi\n",
      "Train Epoch: 4 [83200/263424 (31.56%)]\t Loss: 0.089575 Task: Phish\n",
      "Train Epoch: 4 [89600/263424 (33.99%)]\t Loss: 0.098295 Task: Phish\n",
      "Train Epoch: 4 [96000/263424 (36.42%)]\t Loss: 0.781524 Task: Multi\n",
      "Train Epoch: 4 [102400/263424 (38.85%)]\t Loss: 0.094007 Task: Phish\n",
      "Train Epoch: 4 [108800/263424 (41.28%)]\t Loss: 0.769703 Task: Multi\n",
      "Train Epoch: 4 [115200/263424 (43.71%)]\t Loss: 0.016123 Task: Advertise\n",
      "Train Epoch: 4 [121600/263424 (46.14%)]\t Loss: 0.029071 Task: Phish\n",
      "Train Epoch: 4 [128000/263424 (48.57%)]\t Loss: 0.688085 Task: Multi\n",
      "Train Epoch: 4 [134400/263424 (51.00%)]\t Loss: 0.066064 Task: Phish\n",
      "Train Epoch: 4 [140800/263424 (53.43%)]\t Loss: 0.060052 Task: Phish\n",
      "Train Epoch: 4 [147200/263424 (55.86%)]\t Loss: 0.667370 Task: Multi\n",
      "Train Epoch: 4 [153600/263424 (58.28%)]\t Loss: 0.803712 Task: Multi\n",
      "Train Epoch: 4 [160000/263424 (60.71%)]\t Loss: 1.006834 Task: Multi\n",
      "Train Epoch: 4 [166400/263424 (63.14%)]\t Loss: 0.746804 Task: Multi\n",
      "Train Epoch: 4 [172800/263424 (65.57%)]\t Loss: 0.773101 Task: Multi\n",
      "Train Epoch: 4 [179200/263424 (68.00%)]\t Loss: 0.002825 Task: Advertise\n",
      "Train Epoch: 4 [185600/263424 (70.43%)]\t Loss: 0.779066 Task: Multi\n",
      "Train Epoch: 4 [192000/263424 (72.86%)]\t Loss: 0.095185 Task: Phish\n",
      "Train Epoch: 4 [198400/263424 (75.29%)]\t Loss: 0.080565 Task: Phish\n",
      "Train Epoch: 4 [204800/263424 (77.72%)]\t Loss: 0.002566 Task: Advertise\n",
      "Train Epoch: 4 [211200/263424 (80.15%)]\t Loss: 0.201121 Task: Advertise\n",
      "Train Epoch: 4 [217600/263424 (82.58%)]\t Loss: 0.131085 Task: Phish\n",
      "Train Epoch: 4 [224000/263424 (85.01%)]\t Loss: 0.941035 Task: Multi\n",
      "Train Epoch: 4 [230400/263424 (87.44%)]\t Loss: 0.827576 Task: Multi\n",
      "Train Epoch: 4 [236800/263424 (89.87%)]\t Loss: 0.002428 Task: Advertise\n",
      "Train Epoch: 4 [243200/263424 (92.30%)]\t Loss: 0.168398 Task: Phish\n",
      "Train Epoch: 4 [249600/263424 (94.73%)]\t Loss: 0.097885 Task: Phish\n",
      "Train Epoch: 4 [256000/263424 (97.16%)]\t Loss: 0.691902 Task: Multi\n",
      "Train Epoch: 4 [262400/263424 (99.59%)]\t Loss: 0.004591 Task: Advertise\n",
      "Test set: Task: Phish Average loss: 0.1471, Accuracy: 94.73%, Precision: 94.75%, Recall: 94.73%, F1: 94.73%\n",
      "Test set: Task: Multi Average loss: 0.8241, Accuracy: 70.68%, Precision: 70.72%, Recall: 67.48%, F1: 68.24%\n",
      "Test set: Task: Advertise Average loss: 0.0283, Accuracy: 99.32%, Precision: 99.32%, Recall: 99.32%, F1: 94.73%\n",
      "Phish: acc is: 0.9473, best acc is 0.9484\n",
      "\n",
      "Multi: acc is: 0.7068, best acc is 0.7068\n",
      "\n",
      "Advertise: acc is: 0.9932, best acc is 0.9942\n",
      "\n",
      "Train Epoch: 5 [6400/263424 (2.41%)]\t Loss: 0.071893 Task: Phish\n",
      "Train Epoch: 5 [12800/263424 (4.83%)]\t Loss: 0.081896 Task: Phish\n",
      "Train Epoch: 5 [19200/263424 (7.26%)]\t Loss: 0.829224 Task: Multi\n",
      "Train Epoch: 5 [25600/263424 (9.69%)]\t Loss: 0.882769 Task: Multi\n",
      "Train Epoch: 5 [32000/263424 (12.12%)]\t Loss: 0.705689 Task: Multi\n",
      "Train Epoch: 5 [38400/263424 (14.55%)]\t Loss: 0.695181 Task: Multi\n",
      "Train Epoch: 5 [44800/263424 (16.98%)]\t Loss: 0.789035 Task: Multi\n",
      "Train Epoch: 5 [51200/263424 (19.41%)]\t Loss: 0.833370 Task: Multi\n",
      "Train Epoch: 5 [57600/263424 (21.84%)]\t Loss: 0.683969 Task: Multi\n",
      "Train Epoch: 5 [64000/263424 (24.27%)]\t Loss: 0.003062 Task: Advertise\n",
      "Train Epoch: 5 [70400/263424 (26.70%)]\t Loss: 0.115499 Task: Phish\n",
      "Train Epoch: 5 [76800/263424 (29.13%)]\t Loss: 0.957505 Task: Multi\n",
      "Train Epoch: 5 [83200/263424 (31.56%)]\t Loss: 0.075166 Task: Phish\n",
      "Train Epoch: 5 [89600/263424 (33.99%)]\t Loss: 0.103709 Task: Phish\n",
      "Train Epoch: 5 [96000/263424 (36.42%)]\t Loss: 0.767349 Task: Multi\n",
      "Train Epoch: 5 [102400/263424 (38.85%)]\t Loss: 0.075518 Task: Phish\n",
      "Train Epoch: 5 [108800/263424 (41.28%)]\t Loss: 0.705045 Task: Multi\n",
      "Train Epoch: 5 [115200/263424 (43.71%)]\t Loss: 0.020383 Task: Advertise\n",
      "Train Epoch: 5 [121600/263424 (46.14%)]\t Loss: 0.047579 Task: Phish\n",
      "Train Epoch: 5 [128000/263424 (48.57%)]\t Loss: 0.737468 Task: Multi\n",
      "Train Epoch: 5 [134400/263424 (51.00%)]\t Loss: 0.061310 Task: Phish\n",
      "Train Epoch: 5 [140800/263424 (53.43%)]\t Loss: 0.097164 Task: Phish\n",
      "Train Epoch: 5 [147200/263424 (55.86%)]\t Loss: 0.657640 Task: Multi\n",
      "Train Epoch: 5 [153600/263424 (58.28%)]\t Loss: 0.740608 Task: Multi\n",
      "Train Epoch: 5 [160000/263424 (60.71%)]\t Loss: 0.944634 Task: Multi\n",
      "Train Epoch: 5 [166400/263424 (63.14%)]\t Loss: 0.770519 Task: Multi\n",
      "Train Epoch: 5 [172800/263424 (65.57%)]\t Loss: 0.726354 Task: Multi\n",
      "Train Epoch: 5 [179200/263424 (68.00%)]\t Loss: 0.001370 Task: Advertise\n",
      "Train Epoch: 5 [185600/263424 (70.43%)]\t Loss: 0.751353 Task: Multi\n",
      "Train Epoch: 5 [192000/263424 (72.86%)]\t Loss: 0.098204 Task: Phish\n",
      "Train Epoch: 5 [198400/263424 (75.29%)]\t Loss: 0.114457 Task: Phish\n",
      "Train Epoch: 5 [204800/263424 (77.72%)]\t Loss: 0.001734 Task: Advertise\n",
      "Train Epoch: 5 [211200/263424 (80.15%)]\t Loss: 0.196068 Task: Advertise\n",
      "Train Epoch: 5 [217600/263424 (82.58%)]\t Loss: 0.129853 Task: Phish\n",
      "Train Epoch: 5 [224000/263424 (85.01%)]\t Loss: 0.921849 Task: Multi\n",
      "Train Epoch: 5 [230400/263424 (87.44%)]\t Loss: 0.631468 Task: Multi\n",
      "Train Epoch: 5 [236800/263424 (89.87%)]\t Loss: 0.001670 Task: Advertise\n",
      "Train Epoch: 5 [243200/263424 (92.30%)]\t Loss: 0.165689 Task: Phish\n",
      "Train Epoch: 5 [249600/263424 (94.73%)]\t Loss: 0.119849 Task: Phish\n",
      "Train Epoch: 5 [256000/263424 (97.16%)]\t Loss: 0.690371 Task: Multi\n",
      "Train Epoch: 5 [262400/263424 (99.59%)]\t Loss: 0.002603 Task: Advertise\n",
      "Test set: Task: Phish Average loss: 0.1381, Accuracy: 95.13%, Precision: 95.14%, Recall: 95.12%, F1: 95.13%\n",
      "Test set: Task: Multi Average loss: 0.8161, Accuracy: 71.09%, Precision: 71.15%, Recall: 67.90%, F1: 68.75%\n",
      "Test set: Task: Advertise Average loss: 0.0185, Accuracy: 99.53%, Precision: 99.53%, Recall: 99.53%, F1: 95.13%\n",
      "Phish: acc is: 0.9513, best acc is 0.9513\n",
      "\n",
      "Multi: acc is: 0.7109, best acc is 0.7109\n",
      "\n",
      "Advertise: acc is: 0.9953, best acc is 0.9953\n",
      "\n",
      "Train Epoch: 6 [6400/263424 (2.41%)]\t Loss: 0.024169 Task: Phish\n",
      "Train Epoch: 6 [12800/263424 (4.83%)]\t Loss: 0.074653 Task: Phish\n",
      "Train Epoch: 6 [19200/263424 (7.26%)]\t Loss: 0.871701 Task: Multi\n",
      "Train Epoch: 6 [25600/263424 (9.69%)]\t Loss: 0.814125 Task: Multi\n",
      "Train Epoch: 6 [32000/263424 (12.12%)]\t Loss: 0.697925 Task: Multi\n",
      "Train Epoch: 6 [38400/263424 (14.55%)]\t Loss: 0.688319 Task: Multi\n",
      "Train Epoch: 6 [44800/263424 (16.98%)]\t Loss: 0.767828 Task: Multi\n",
      "Train Epoch: 6 [51200/263424 (19.41%)]\t Loss: 0.769189 Task: Multi\n",
      "Train Epoch: 6 [57600/263424 (21.84%)]\t Loss: 0.700058 Task: Multi\n",
      "Train Epoch: 6 [64000/263424 (24.27%)]\t Loss: 0.001942 Task: Advertise\n",
      "Train Epoch: 6 [70400/263424 (26.70%)]\t Loss: 0.173221 Task: Phish\n",
      "Train Epoch: 6 [76800/263424 (29.13%)]\t Loss: 0.858097 Task: Multi\n",
      "Train Epoch: 6 [83200/263424 (31.56%)]\t Loss: 0.041431 Task: Phish\n",
      "Train Epoch: 6 [89600/263424 (33.99%)]\t Loss: 0.086390 Task: Phish\n",
      "Train Epoch: 6 [96000/263424 (36.42%)]\t Loss: 0.773894 Task: Multi\n",
      "Train Epoch: 6 [102400/263424 (38.85%)]\t Loss: 0.074303 Task: Phish\n",
      "Train Epoch: 6 [108800/263424 (41.28%)]\t Loss: 0.693051 Task: Multi\n",
      "Train Epoch: 6 [115200/263424 (43.71%)]\t Loss: 0.044264 Task: Advertise\n",
      "Train Epoch: 6 [121600/263424 (46.14%)]\t Loss: 0.027291 Task: Phish\n",
      "Train Epoch: 6 [128000/263424 (48.57%)]\t Loss: 0.715483 Task: Multi\n",
      "Train Epoch: 6 [134400/263424 (51.00%)]\t Loss: 0.049817 Task: Phish\n",
      "Train Epoch: 6 [140800/263424 (53.43%)]\t Loss: 0.091833 Task: Phish\n",
      "Train Epoch: 6 [147200/263424 (55.86%)]\t Loss: 0.668663 Task: Multi\n",
      "Train Epoch: 6 [153600/263424 (58.28%)]\t Loss: 0.718033 Task: Multi\n",
      "Train Epoch: 6 [160000/263424 (60.71%)]\t Loss: 0.929894 Task: Multi\n",
      "Train Epoch: 6 [166400/263424 (63.14%)]\t Loss: 0.681238 Task: Multi\n",
      "Train Epoch: 6 [172800/263424 (65.57%)]\t Loss: 0.686114 Task: Multi\n",
      "Train Epoch: 6 [179200/263424 (68.00%)]\t Loss: 0.001708 Task: Advertise\n",
      "Train Epoch: 6 [185600/263424 (70.43%)]\t Loss: 0.672606 Task: Multi\n",
      "Train Epoch: 6 [192000/263424 (72.86%)]\t Loss: 0.067509 Task: Phish\n",
      "Train Epoch: 6 [198400/263424 (75.29%)]\t Loss: 0.024555 Task: Phish\n",
      "Train Epoch: 6 [204800/263424 (77.72%)]\t Loss: 0.002859 Task: Advertise\n",
      "Train Epoch: 6 [211200/263424 (80.15%)]\t Loss: 0.180764 Task: Advertise\n",
      "Train Epoch: 6 [217600/263424 (82.58%)]\t Loss: 0.111628 Task: Phish\n",
      "Train Epoch: 6 [224000/263424 (85.01%)]\t Loss: 0.781469 Task: Multi\n",
      "Train Epoch: 6 [230400/263424 (87.44%)]\t Loss: 0.640651 Task: Multi\n",
      "Train Epoch: 6 [236800/263424 (89.87%)]\t Loss: 0.001745 Task: Advertise\n",
      "Train Epoch: 6 [243200/263424 (92.30%)]\t Loss: 0.133621 Task: Phish\n",
      "Train Epoch: 6 [249600/263424 (94.73%)]\t Loss: 0.097132 Task: Phish\n",
      "Train Epoch: 6 [256000/263424 (97.16%)]\t Loss: 0.710592 Task: Multi\n",
      "Train Epoch: 6 [262400/263424 (99.59%)]\t Loss: 0.003326 Task: Advertise\n",
      "Test set: Task: Phish Average loss: 0.1313, Accuracy: 95.34%, Precision: 95.35%, Recall: 95.35%, F1: 95.34%\n",
      "Test set: Task: Multi Average loss: 0.8092, Accuracy: 71.48%, Precision: 71.36%, Recall: 68.39%, F1: 69.32%\n",
      "Test set: Task: Advertise Average loss: 0.0198, Accuracy: 99.56%, Precision: 99.56%, Recall: 99.56%, F1: 95.34%\n",
      "Phish: acc is: 0.9534, best acc is 0.9534\n",
      "\n",
      "Multi: acc is: 0.7148, best acc is 0.7148\n",
      "\n",
      "Advertise: acc is: 0.9956, best acc is 0.9956\n",
      "\n",
      "Train Epoch: 7 [6400/263424 (2.41%)]\t Loss: 0.027123 Task: Phish\n",
      "Train Epoch: 7 [12800/263424 (4.83%)]\t Loss: 0.056608 Task: Phish\n",
      "Train Epoch: 7 [19200/263424 (7.26%)]\t Loss: 0.813237 Task: Multi\n",
      "Train Epoch: 7 [25600/263424 (9.69%)]\t Loss: 0.834771 Task: Multi\n",
      "Train Epoch: 7 [32000/263424 (12.12%)]\t Loss: 0.612522 Task: Multi\n",
      "Train Epoch: 7 [38400/263424 (14.55%)]\t Loss: 0.640846 Task: Multi\n",
      "Train Epoch: 7 [44800/263424 (16.98%)]\t Loss: 0.735150 Task: Multi\n",
      "Train Epoch: 7 [51200/263424 (19.41%)]\t Loss: 0.731496 Task: Multi\n",
      "Train Epoch: 7 [57600/263424 (21.84%)]\t Loss: 0.616938 Task: Multi\n",
      "Train Epoch: 7 [64000/263424 (24.27%)]\t Loss: 0.002013 Task: Advertise\n",
      "Train Epoch: 7 [70400/263424 (26.70%)]\t Loss: 0.052541 Task: Phish\n",
      "Train Epoch: 7 [76800/263424 (29.13%)]\t Loss: 0.950193 Task: Multi\n",
      "Train Epoch: 7 [83200/263424 (31.56%)]\t Loss: 0.046878 Task: Phish\n",
      "Train Epoch: 7 [89600/263424 (33.99%)]\t Loss: 0.083137 Task: Phish\n",
      "Train Epoch: 7 [96000/263424 (36.42%)]\t Loss: 0.678857 Task: Multi\n",
      "Train Epoch: 7 [102400/263424 (38.85%)]\t Loss: 0.038840 Task: Phish\n",
      "Train Epoch: 7 [108800/263424 (41.28%)]\t Loss: 0.691078 Task: Multi\n",
      "Train Epoch: 7 [115200/263424 (43.71%)]\t Loss: 0.002518 Task: Advertise\n",
      "Train Epoch: 7 [121600/263424 (46.14%)]\t Loss: 0.013998 Task: Phish\n",
      "Train Epoch: 7 [128000/263424 (48.57%)]\t Loss: 0.680837 Task: Multi\n",
      "Train Epoch: 7 [134400/263424 (51.00%)]\t Loss: 0.070521 Task: Phish\n",
      "Train Epoch: 7 [140800/263424 (53.43%)]\t Loss: 0.019499 Task: Phish\n",
      "Train Epoch: 7 [147200/263424 (55.86%)]\t Loss: 0.577470 Task: Multi\n",
      "Train Epoch: 7 [153600/263424 (58.28%)]\t Loss: 0.643671 Task: Multi\n",
      "Train Epoch: 7 [160000/263424 (60.71%)]\t Loss: 0.978936 Task: Multi\n",
      "Train Epoch: 7 [166400/263424 (63.14%)]\t Loss: 0.630598 Task: Multi\n",
      "Train Epoch: 7 [172800/263424 (65.57%)]\t Loss: 0.685349 Task: Multi\n",
      "Train Epoch: 7 [179200/263424 (68.00%)]\t Loss: 0.001009 Task: Advertise\n",
      "Train Epoch: 7 [185600/263424 (70.43%)]\t Loss: 0.587138 Task: Multi\n",
      "Train Epoch: 7 [192000/263424 (72.86%)]\t Loss: 0.108055 Task: Phish\n",
      "Train Epoch: 7 [198400/263424 (75.29%)]\t Loss: 0.064669 Task: Phish\n",
      "Train Epoch: 7 [204800/263424 (77.72%)]\t Loss: 0.003458 Task: Advertise\n",
      "Train Epoch: 7 [211200/263424 (80.15%)]\t Loss: 0.220267 Task: Advertise\n",
      "Train Epoch: 7 [217600/263424 (82.58%)]\t Loss: 0.144023 Task: Phish\n",
      "Train Epoch: 7 [224000/263424 (85.01%)]\t Loss: 0.749261 Task: Multi\n",
      "Train Epoch: 7 [230400/263424 (87.44%)]\t Loss: 0.590188 Task: Multi\n",
      "Train Epoch: 7 [236800/263424 (89.87%)]\t Loss: 0.001066 Task: Advertise\n",
      "Train Epoch: 7 [243200/263424 (92.30%)]\t Loss: 0.087271 Task: Phish\n",
      "Train Epoch: 7 [249600/263424 (94.73%)]\t Loss: 0.111722 Task: Phish\n",
      "Train Epoch: 7 [256000/263424 (97.16%)]\t Loss: 0.644503 Task: Multi\n",
      "Train Epoch: 7 [262400/263424 (99.59%)]\t Loss: 0.003670 Task: Advertise\n",
      "Test set: Task: Phish Average loss: 0.1390, Accuracy: 95.44%, Precision: 95.46%, Recall: 95.45%, F1: 95.44%\n",
      "Test set: Task: Multi Average loss: 0.8139, Accuracy: 71.55%, Precision: 71.66%, Recall: 68.42%, F1: 69.45%\n",
      "Test set: Task: Advertise Average loss: 0.0178, Accuracy: 99.53%, Precision: 99.53%, Recall: 99.53%, F1: 95.44%\n",
      "Phish: acc is: 0.9544, best acc is 0.9544\n",
      "\n",
      "Multi: acc is: 0.7155, best acc is 0.7155\n",
      "\n",
      "Advertise: acc is: 0.9953, best acc is 0.9956\n",
      "\n",
      "Train Epoch: 8 [6400/263424 (2.41%)]\t Loss: 0.055961 Task: Phish\n",
      "Train Epoch: 8 [12800/263424 (4.83%)]\t Loss: 0.053479 Task: Phish\n",
      "Train Epoch: 8 [19200/263424 (7.26%)]\t Loss: 0.803622 Task: Multi\n",
      "Train Epoch: 8 [25600/263424 (9.69%)]\t Loss: 0.728975 Task: Multi\n",
      "Train Epoch: 8 [32000/263424 (12.12%)]\t Loss: 0.510537 Task: Multi\n",
      "Train Epoch: 8 [38400/263424 (14.55%)]\t Loss: 0.569409 Task: Multi\n",
      "Train Epoch: 8 [44800/263424 (16.98%)]\t Loss: 0.703264 Task: Multi\n",
      "Train Epoch: 8 [51200/263424 (19.41%)]\t Loss: 0.694090 Task: Multi\n",
      "Train Epoch: 8 [57600/263424 (21.84%)]\t Loss: 0.529796 Task: Multi\n",
      "Train Epoch: 8 [64000/263424 (24.27%)]\t Loss: 0.001787 Task: Advertise\n",
      "Train Epoch: 8 [70400/263424 (26.70%)]\t Loss: 0.051298 Task: Phish\n",
      "Train Epoch: 8 [76800/263424 (29.13%)]\t Loss: 0.922783 Task: Multi\n",
      "Train Epoch: 8 [83200/263424 (31.56%)]\t Loss: 0.046670 Task: Phish\n",
      "Train Epoch: 8 [89600/263424 (33.99%)]\t Loss: 0.033303 Task: Phish\n",
      "Train Epoch: 8 [96000/263424 (36.42%)]\t Loss: 0.639244 Task: Multi\n",
      "Train Epoch: 8 [102400/263424 (38.85%)]\t Loss: 0.036092 Task: Phish\n",
      "Train Epoch: 8 [108800/263424 (41.28%)]\t Loss: 0.658526 Task: Multi\n",
      "Train Epoch: 8 [115200/263424 (43.71%)]\t Loss: 0.023597 Task: Advertise\n",
      "Train Epoch: 8 [121600/263424 (46.14%)]\t Loss: 0.028842 Task: Phish\n",
      "Train Epoch: 8 [128000/263424 (48.57%)]\t Loss: 0.708032 Task: Multi\n",
      "Train Epoch: 8 [134400/263424 (51.00%)]\t Loss: 0.064695 Task: Phish\n",
      "Train Epoch: 8 [140800/263424 (53.43%)]\t Loss: 0.023451 Task: Phish\n",
      "Train Epoch: 8 [147200/263424 (55.86%)]\t Loss: 0.551211 Task: Multi\n",
      "Train Epoch: 8 [153600/263424 (58.28%)]\t Loss: 0.653197 Task: Multi\n",
      "Train Epoch: 8 [160000/263424 (60.71%)]\t Loss: 0.875922 Task: Multi\n",
      "Train Epoch: 8 [166400/263424 (63.14%)]\t Loss: 0.699384 Task: Multi\n",
      "Train Epoch: 8 [172800/263424 (65.57%)]\t Loss: 0.677471 Task: Multi\n",
      "Train Epoch: 8 [179200/263424 (68.00%)]\t Loss: 0.001104 Task: Advertise\n",
      "Train Epoch: 8 [185600/263424 (70.43%)]\t Loss: 0.579041 Task: Multi\n",
      "Train Epoch: 8 [192000/263424 (72.86%)]\t Loss: 0.091509 Task: Phish\n",
      "Train Epoch: 8 [198400/263424 (75.29%)]\t Loss: 0.037152 Task: Phish\n",
      "Train Epoch: 8 [204800/263424 (77.72%)]\t Loss: 0.020684 Task: Advertise\n",
      "Train Epoch: 8 [211200/263424 (80.15%)]\t Loss: 0.140559 Task: Advertise\n",
      "Train Epoch: 8 [217600/263424 (82.58%)]\t Loss: 0.103768 Task: Phish\n",
      "Train Epoch: 8 [224000/263424 (85.01%)]\t Loss: 0.857663 Task: Multi\n",
      "Train Epoch: 8 [230400/263424 (87.44%)]\t Loss: 0.565513 Task: Multi\n",
      "Train Epoch: 8 [236800/263424 (89.87%)]\t Loss: 0.001034 Task: Advertise\n",
      "Train Epoch: 8 [243200/263424 (92.30%)]\t Loss: 0.084820 Task: Phish\n",
      "Train Epoch: 8 [249600/263424 (94.73%)]\t Loss: 0.114086 Task: Phish\n",
      "Train Epoch: 8 [256000/263424 (97.16%)]\t Loss: 0.616205 Task: Multi\n",
      "Train Epoch: 8 [262400/263424 (99.59%)]\t Loss: 0.001250 Task: Advertise\n",
      "Test set: Task: Phish Average loss: 0.1458, Accuracy: 95.31%, Precision: 95.33%, Recall: 95.31%, F1: 95.31%\n",
      "Test set: Task: Multi Average loss: 0.8160, Accuracy: 71.75%, Precision: 71.72%, Recall: 68.79%, F1: 69.76%\n",
      "Test set: Task: Advertise Average loss: 0.0234, Accuracy: 99.61%, Precision: 99.61%, Recall: 99.61%, F1: 95.31%\n",
      "Phish: acc is: 0.9531, best acc is 0.9544\n",
      "\n",
      "Multi: acc is: 0.7175, best acc is 0.7175\n",
      "\n",
      "Advertise: acc is: 0.9961, best acc is 0.9961\n",
      "\n",
      "Train Epoch: 9 [6400/263424 (2.41%)]\t Loss: 0.049110 Task: Phish\n",
      "Train Epoch: 9 [12800/263424 (4.83%)]\t Loss: 0.119724 Task: Phish\n",
      "Train Epoch: 9 [19200/263424 (7.26%)]\t Loss: 0.750779 Task: Multi\n",
      "Train Epoch: 9 [25600/263424 (9.69%)]\t Loss: 0.708318 Task: Multi\n",
      "Train Epoch: 9 [32000/263424 (12.12%)]\t Loss: 0.457132 Task: Multi\n",
      "Train Epoch: 9 [38400/263424 (14.55%)]\t Loss: 0.601005 Task: Multi\n",
      "Train Epoch: 9 [44800/263424 (16.98%)]\t Loss: 0.697511 Task: Multi\n",
      "Train Epoch: 9 [51200/263424 (19.41%)]\t Loss: 0.666805 Task: Multi\n",
      "Train Epoch: 9 [57600/263424 (21.84%)]\t Loss: 0.492813 Task: Multi\n",
      "Train Epoch: 9 [64000/263424 (24.27%)]\t Loss: 0.011470 Task: Advertise\n",
      "Train Epoch: 9 [70400/263424 (26.70%)]\t Loss: 0.031995 Task: Phish\n",
      "Train Epoch: 9 [76800/263424 (29.13%)]\t Loss: 0.909638 Task: Multi\n",
      "Train Epoch: 9 [83200/263424 (31.56%)]\t Loss: 0.025480 Task: Phish\n",
      "Train Epoch: 9 [89600/263424 (33.99%)]\t Loss: 0.058053 Task: Phish\n",
      "Train Epoch: 9 [96000/263424 (36.42%)]\t Loss: 0.615545 Task: Multi\n",
      "Train Epoch: 9 [102400/263424 (38.85%)]\t Loss: 0.050029 Task: Phish\n",
      "Train Epoch: 9 [108800/263424 (41.28%)]\t Loss: 0.563482 Task: Multi\n",
      "Train Epoch: 9 [115200/263424 (43.71%)]\t Loss: 0.001543 Task: Advertise\n",
      "Train Epoch: 9 [121600/263424 (46.14%)]\t Loss: 0.077929 Task: Phish\n",
      "Train Epoch: 9 [128000/263424 (48.57%)]\t Loss: 0.718820 Task: Multi\n",
      "Train Epoch: 9 [134400/263424 (51.00%)]\t Loss: 0.027376 Task: Phish\n",
      "Train Epoch: 9 [140800/263424 (53.43%)]\t Loss: 0.016250 Task: Phish\n",
      "Train Epoch: 9 [147200/263424 (55.86%)]\t Loss: 0.552228 Task: Multi\n",
      "Train Epoch: 9 [153600/263424 (58.28%)]\t Loss: 0.615021 Task: Multi\n",
      "Train Epoch: 9 [160000/263424 (60.71%)]\t Loss: 0.744378 Task: Multi\n",
      "Train Epoch: 9 [166400/263424 (63.14%)]\t Loss: 0.688027 Task: Multi\n",
      "Train Epoch: 9 [172800/263424 (65.57%)]\t Loss: 0.620870 Task: Multi\n",
      "Train Epoch: 9 [179200/263424 (68.00%)]\t Loss: 0.000666 Task: Advertise\n",
      "Train Epoch: 9 [185600/263424 (70.43%)]\t Loss: 0.542568 Task: Multi\n",
      "Train Epoch: 9 [192000/263424 (72.86%)]\t Loss: 0.084569 Task: Phish\n",
      "Train Epoch: 9 [198400/263424 (75.29%)]\t Loss: 0.021818 Task: Phish\n",
      "Train Epoch: 9 [204800/263424 (77.72%)]\t Loss: 0.000685 Task: Advertise\n",
      "Train Epoch: 9 [211200/263424 (80.15%)]\t Loss: 0.074459 Task: Advertise\n",
      "Train Epoch: 9 [217600/263424 (82.58%)]\t Loss: 0.093397 Task: Phish\n",
      "Train Epoch: 9 [224000/263424 (85.01%)]\t Loss: 0.776305 Task: Multi\n",
      "Train Epoch: 9 [230400/263424 (87.44%)]\t Loss: 0.548516 Task: Multi\n",
      "Train Epoch: 9 [236800/263424 (89.87%)]\t Loss: 0.000800 Task: Advertise\n",
      "Train Epoch: 9 [243200/263424 (92.30%)]\t Loss: 0.090883 Task: Phish\n",
      "Train Epoch: 9 [249600/263424 (94.73%)]\t Loss: 0.109496 Task: Phish\n",
      "Train Epoch: 9 [256000/263424 (97.16%)]\t Loss: 0.668632 Task: Multi\n",
      "Train Epoch: 9 [262400/263424 (99.59%)]\t Loss: 0.002147 Task: Advertise\n",
      "Test set: Task: Phish Average loss: 0.1554, Accuracy: 94.94%, Precision: 95.10%, Recall: 94.97%, F1: 94.94%\n",
      "Test set: Task: Multi Average loss: 0.8237, Accuracy: 71.71%, Precision: 71.65%, Recall: 68.81%, F1: 69.79%\n",
      "Test set: Task: Advertise Average loss: 0.0139, Accuracy: 99.66%, Precision: 99.66%, Recall: 99.66%, F1: 94.94%\n",
      "Phish: acc is: 0.9494, best acc is 0.9544\n",
      "\n",
      "Multi: acc is: 0.7171, best acc is 0.7175\n",
      "\n",
      "Advertise: acc is: 0.9966, best acc is 0.9966\n",
      "\n",
      "Train Epoch: 10 [6400/263424 (2.41%)]\t Loss: 0.023077 Task: Phish\n",
      "Train Epoch: 10 [12800/263424 (4.83%)]\t Loss: 0.039078 Task: Phish\n",
      "Train Epoch: 10 [19200/263424 (7.26%)]\t Loss: 0.727886 Task: Multi\n",
      "Train Epoch: 10 [25600/263424 (9.69%)]\t Loss: 0.671423 Task: Multi\n",
      "Train Epoch: 10 [32000/263424 (12.12%)]\t Loss: 0.435180 Task: Multi\n",
      "Train Epoch: 10 [38400/263424 (14.55%)]\t Loss: 0.509107 Task: Multi\n",
      "Train Epoch: 10 [44800/263424 (16.98%)]\t Loss: 0.653244 Task: Multi\n",
      "Train Epoch: 10 [51200/263424 (19.41%)]\t Loss: 0.688362 Task: Multi\n",
      "Train Epoch: 10 [57600/263424 (21.84%)]\t Loss: 0.511726 Task: Multi\n",
      "Train Epoch: 10 [64000/263424 (24.27%)]\t Loss: 0.001800 Task: Advertise\n",
      "Train Epoch: 10 [70400/263424 (26.70%)]\t Loss: 0.090890 Task: Phish\n",
      "Train Epoch: 10 [76800/263424 (29.13%)]\t Loss: 0.833411 Task: Multi\n",
      "Train Epoch: 10 [83200/263424 (31.56%)]\t Loss: 0.037512 Task: Phish\n",
      "Train Epoch: 10 [89600/263424 (33.99%)]\t Loss: 0.047109 Task: Phish\n",
      "Train Epoch: 10 [96000/263424 (36.42%)]\t Loss: 0.590800 Task: Multi\n",
      "Train Epoch: 10 [102400/263424 (38.85%)]\t Loss: 0.011803 Task: Phish\n",
      "Train Epoch: 10 [108800/263424 (41.28%)]\t Loss: 0.474362 Task: Multi\n",
      "Train Epoch: 10 [115200/263424 (43.71%)]\t Loss: 0.040486 Task: Advertise\n",
      "Train Epoch: 10 [121600/263424 (46.14%)]\t Loss: 0.008619 Task: Phish\n",
      "Train Epoch: 10 [128000/263424 (48.57%)]\t Loss: 0.621791 Task: Multi\n",
      "Train Epoch: 10 [134400/263424 (51.00%)]\t Loss: 0.038220 Task: Phish\n",
      "Train Epoch: 10 [140800/263424 (53.43%)]\t Loss: 0.020696 Task: Phish\n",
      "Train Epoch: 10 [147200/263424 (55.86%)]\t Loss: 0.479323 Task: Multi\n",
      "Train Epoch: 10 [153600/263424 (58.28%)]\t Loss: 0.592552 Task: Multi\n",
      "Train Epoch: 10 [160000/263424 (60.71%)]\t Loss: 0.863541 Task: Multi\n",
      "Train Epoch: 10 [166400/263424 (63.14%)]\t Loss: 0.631215 Task: Multi\n",
      "Train Epoch: 10 [172800/263424 (65.57%)]\t Loss: 0.582782 Task: Multi\n",
      "Train Epoch: 10 [179200/263424 (68.00%)]\t Loss: 0.000910 Task: Advertise\n",
      "Train Epoch: 10 [185600/263424 (70.43%)]\t Loss: 0.468927 Task: Multi\n",
      "Train Epoch: 10 [192000/263424 (72.86%)]\t Loss: 0.067893 Task: Phish\n",
      "Train Epoch: 10 [198400/263424 (75.29%)]\t Loss: 0.005787 Task: Phish\n",
      "Train Epoch: 10 [204800/263424 (77.72%)]\t Loss: 0.004765 Task: Advertise\n",
      "Train Epoch: 10 [211200/263424 (80.15%)]\t Loss: 0.008028 Task: Advertise\n",
      "Train Epoch: 10 [217600/263424 (82.58%)]\t Loss: 0.058044 Task: Phish\n",
      "Train Epoch: 10 [224000/263424 (85.01%)]\t Loss: 0.743693 Task: Multi\n",
      "Train Epoch: 10 [230400/263424 (87.44%)]\t Loss: 0.489832 Task: Multi\n",
      "Train Epoch: 10 [236800/263424 (89.87%)]\t Loss: 0.000814 Task: Advertise\n",
      "Train Epoch: 10 [243200/263424 (92.30%)]\t Loss: 0.092157 Task: Phish\n",
      "Train Epoch: 10 [249600/263424 (94.73%)]\t Loss: 0.026213 Task: Phish\n",
      "Train Epoch: 10 [256000/263424 (97.16%)]\t Loss: 0.591955 Task: Multi\n",
      "Train Epoch: 10 [262400/263424 (99.59%)]\t Loss: 0.001004 Task: Advertise\n",
      "Test set: Task: Phish Average loss: 0.1452, Accuracy: 95.47%, Precision: 95.48%, Recall: 95.47%, F1: 95.47%\n",
      "Test set: Task: Multi Average loss: 0.8432, Accuracy: 71.68%, Precision: 71.60%, Recall: 68.82%, F1: 69.76%\n",
      "Test set: Task: Advertise Average loss: 0.0170, Accuracy: 99.62%, Precision: 99.62%, Recall: 99.62%, F1: 95.47%\n",
      "Phish: acc is: 0.9547, best acc is 0.9547\n",
      "\n",
      "Multi: acc is: 0.7168, best acc is 0.7175\n",
      "\n",
      "Advertise: acc is: 0.9962, best acc is 0.9966\n",
      "\n",
      "Train Epoch: 11 [6400/263424 (2.41%)]\t Loss: 0.006727 Task: Phish\n",
      "Train Epoch: 11 [12800/263424 (4.83%)]\t Loss: 0.076446 Task: Phish\n",
      "Train Epoch: 11 [19200/263424 (7.26%)]\t Loss: 0.605870 Task: Multi\n",
      "Train Epoch: 11 [25600/263424 (9.69%)]\t Loss: 0.633725 Task: Multi\n",
      "Train Epoch: 11 [32000/263424 (12.12%)]\t Loss: 0.369652 Task: Multi\n",
      "Train Epoch: 11 [38400/263424 (14.55%)]\t Loss: 0.565917 Task: Multi\n",
      "Train Epoch: 11 [44800/263424 (16.98%)]\t Loss: 0.589149 Task: Multi\n",
      "Train Epoch: 11 [51200/263424 (19.41%)]\t Loss: 0.597533 Task: Multi\n",
      "Train Epoch: 11 [57600/263424 (21.84%)]\t Loss: 0.447269 Task: Multi\n",
      "Train Epoch: 11 [64000/263424 (24.27%)]\t Loss: 0.000352 Task: Advertise\n",
      "Train Epoch: 11 [70400/263424 (26.70%)]\t Loss: 0.008103 Task: Phish\n",
      "Train Epoch: 11 [76800/263424 (29.13%)]\t Loss: 0.802214 Task: Multi\n",
      "Train Epoch: 11 [83200/263424 (31.56%)]\t Loss: 0.010476 Task: Phish\n",
      "Train Epoch: 11 [89600/263424 (33.99%)]\t Loss: 0.083322 Task: Phish\n",
      "Train Epoch: 11 [96000/263424 (36.42%)]\t Loss: 0.539776 Task: Multi\n",
      "Train Epoch: 11 [102400/263424 (38.85%)]\t Loss: 0.037181 Task: Phish\n",
      "Train Epoch: 11 [108800/263424 (41.28%)]\t Loss: 0.534122 Task: Multi\n",
      "Train Epoch: 11 [115200/263424 (43.71%)]\t Loss: 0.001437 Task: Advertise\n",
      "Train Epoch: 11 [121600/263424 (46.14%)]\t Loss: 0.012400 Task: Phish\n",
      "Train Epoch: 11 [128000/263424 (48.57%)]\t Loss: 0.638775 Task: Multi\n",
      "Train Epoch: 11 [134400/263424 (51.00%)]\t Loss: 0.039731 Task: Phish\n",
      "Train Epoch: 11 [140800/263424 (53.43%)]\t Loss: 0.026076 Task: Phish\n",
      "Train Epoch: 11 [147200/263424 (55.86%)]\t Loss: 0.483447 Task: Multi\n",
      "Train Epoch: 11 [153600/263424 (58.28%)]\t Loss: 0.610819 Task: Multi\n",
      "Train Epoch: 11 [160000/263424 (60.71%)]\t Loss: 0.797890 Task: Multi\n",
      "Train Epoch: 11 [166400/263424 (63.14%)]\t Loss: 0.674473 Task: Multi\n",
      "Train Epoch: 11 [172800/263424 (65.57%)]\t Loss: 0.566586 Task: Multi\n",
      "Train Epoch: 11 [179200/263424 (68.00%)]\t Loss: 0.000526 Task: Advertise\n",
      "Train Epoch: 11 [185600/263424 (70.43%)]\t Loss: 0.518295 Task: Multi\n",
      "Train Epoch: 11 [192000/263424 (72.86%)]\t Loss: 0.126171 Task: Phish\n",
      "Train Epoch: 11 [198400/263424 (75.29%)]\t Loss: 0.010201 Task: Phish\n",
      "Train Epoch: 11 [204800/263424 (77.72%)]\t Loss: 0.000402 Task: Advertise\n",
      "Train Epoch: 11 [211200/263424 (80.15%)]\t Loss: 0.018639 Task: Advertise\n",
      "Train Epoch: 11 [217600/263424 (82.58%)]\t Loss: 0.042909 Task: Phish\n",
      "Train Epoch: 11 [224000/263424 (85.01%)]\t Loss: 0.809478 Task: Multi\n",
      "Train Epoch: 11 [230400/263424 (87.44%)]\t Loss: 0.591778 Task: Multi\n",
      "Train Epoch: 11 [236800/263424 (89.87%)]\t Loss: 0.000606 Task: Advertise\n",
      "Train Epoch: 11 [243200/263424 (92.30%)]\t Loss: 0.048192 Task: Phish\n",
      "Train Epoch: 11 [249600/263424 (94.73%)]\t Loss: 0.156579 Task: Phish\n",
      "Train Epoch: 11 [256000/263424 (97.16%)]\t Loss: 0.529829 Task: Multi\n",
      "Train Epoch: 11 [262400/263424 (99.59%)]\t Loss: 0.007119 Task: Advertise\n",
      "Test set: Task: Phish Average loss: 0.1504, Accuracy: 95.40%, Precision: 95.44%, Recall: 95.41%, F1: 95.40%\n",
      "Test set: Task: Multi Average loss: 0.8421, Accuracy: 71.89%, Precision: 71.37%, Recall: 69.33%, F1: 70.07%\n",
      "Test set: Task: Advertise Average loss: 0.0172, Accuracy: 99.71%, Precision: 99.71%, Recall: 99.71%, F1: 95.40%\n",
      "Phish: acc is: 0.9540, best acc is 0.9547\n",
      "\n",
      "Multi: acc is: 0.7189, best acc is 0.7189\n",
      "\n",
      "Advertise: acc is: 0.9971, best acc is 0.9971\n",
      "\n",
      "Train Epoch: 12 [6400/263424 (2.41%)]\t Loss: 0.004734 Task: Phish\n",
      "Train Epoch: 12 [12800/263424 (4.83%)]\t Loss: 0.057006 Task: Phish\n",
      "Train Epoch: 12 [19200/263424 (7.26%)]\t Loss: 0.679720 Task: Multi\n",
      "Train Epoch: 12 [25600/263424 (9.69%)]\t Loss: 0.709110 Task: Multi\n",
      "Train Epoch: 12 [32000/263424 (12.12%)]\t Loss: 0.346904 Task: Multi\n",
      "Train Epoch: 12 [38400/263424 (14.55%)]\t Loss: 0.471190 Task: Multi\n",
      "Train Epoch: 12 [44800/263424 (16.98%)]\t Loss: 0.650640 Task: Multi\n",
      "Train Epoch: 12 [51200/263424 (19.41%)]\t Loss: 0.577262 Task: Multi\n",
      "Train Epoch: 12 [57600/263424 (21.84%)]\t Loss: 0.359790 Task: Multi\n",
      "Train Epoch: 12 [64000/263424 (24.27%)]\t Loss: 0.000414 Task: Advertise\n",
      "Train Epoch: 12 [70400/263424 (26.70%)]\t Loss: 0.005740 Task: Phish\n",
      "Train Epoch: 12 [76800/263424 (29.13%)]\t Loss: 0.872775 Task: Multi\n",
      "Train Epoch: 12 [83200/263424 (31.56%)]\t Loss: 0.016426 Task: Phish\n",
      "Train Epoch: 12 [89600/263424 (33.99%)]\t Loss: 0.006677 Task: Phish\n",
      "Train Epoch: 12 [96000/263424 (36.42%)]\t Loss: 0.475362 Task: Multi\n",
      "Train Epoch: 12 [102400/263424 (38.85%)]\t Loss: 0.034278 Task: Phish\n",
      "Train Epoch: 12 [108800/263424 (41.28%)]\t Loss: 0.459135 Task: Multi\n",
      "Train Epoch: 12 [115200/263424 (43.71%)]\t Loss: 0.000735 Task: Advertise\n",
      "Train Epoch: 12 [121600/263424 (46.14%)]\t Loss: 0.015646 Task: Phish\n",
      "Train Epoch: 12 [128000/263424 (48.57%)]\t Loss: 0.594225 Task: Multi\n",
      "Train Epoch: 12 [134400/263424 (51.00%)]\t Loss: 0.019223 Task: Phish\n",
      "Train Epoch: 12 [140800/263424 (53.43%)]\t Loss: 0.008605 Task: Phish\n",
      "Train Epoch: 12 [147200/263424 (55.86%)]\t Loss: 0.478887 Task: Multi\n",
      "Train Epoch: 12 [153600/263424 (58.28%)]\t Loss: 0.496056 Task: Multi\n",
      "Train Epoch: 12 [160000/263424 (60.71%)]\t Loss: 0.716266 Task: Multi\n",
      "Train Epoch: 12 [166400/263424 (63.14%)]\t Loss: 0.605656 Task: Multi\n",
      "Train Epoch: 12 [172800/263424 (65.57%)]\t Loss: 0.462609 Task: Multi\n",
      "Train Epoch: 12 [179200/263424 (68.00%)]\t Loss: 0.000481 Task: Advertise\n",
      "Train Epoch: 12 [185600/263424 (70.43%)]\t Loss: 0.492158 Task: Multi\n",
      "Train Epoch: 12 [192000/263424 (72.86%)]\t Loss: 0.090999 Task: Phish\n",
      "Train Epoch: 12 [198400/263424 (75.29%)]\t Loss: 0.001871 Task: Phish\n",
      "Train Epoch: 12 [204800/263424 (77.72%)]\t Loss: 0.000475 Task: Advertise\n",
      "Train Epoch: 12 [211200/263424 (80.15%)]\t Loss: 0.007432 Task: Advertise\n",
      "Train Epoch: 12 [217600/263424 (82.58%)]\t Loss: 0.084345 Task: Phish\n",
      "Train Epoch: 12 [224000/263424 (85.01%)]\t Loss: 0.766973 Task: Multi\n",
      "Train Epoch: 12 [230400/263424 (87.44%)]\t Loss: 0.432514 Task: Multi\n",
      "Train Epoch: 12 [236800/263424 (89.87%)]\t Loss: 0.000381 Task: Advertise\n",
      "Train Epoch: 12 [243200/263424 (92.30%)]\t Loss: 0.109406 Task: Phish\n",
      "Train Epoch: 12 [249600/263424 (94.73%)]\t Loss: 0.031256 Task: Phish\n",
      "Train Epoch: 12 [256000/263424 (97.16%)]\t Loss: 0.592073 Task: Multi\n",
      "Train Epoch: 12 [262400/263424 (99.59%)]\t Loss: 0.015630 Task: Advertise\n",
      "Test set: Task: Phish Average loss: 0.1803, Accuracy: 95.32%, Precision: 95.33%, Recall: 95.31%, F1: 95.32%\n",
      "Test set: Task: Multi Average loss: 0.8834, Accuracy: 71.50%, Precision: 71.36%, Recall: 68.73%, F1: 69.61%\n",
      "Test set: Task: Advertise Average loss: 0.0210, Accuracy: 99.63%, Precision: 99.63%, Recall: 99.63%, F1: 95.32%\n",
      "Phish: acc is: 0.9532, best acc is 0.9547\n",
      "\n",
      "Multi: acc is: 0.7150, best acc is 0.7189\n",
      "\n",
      "Advertise: acc is: 0.9963, best acc is 0.9971\n",
      "\n",
      "Train Epoch: 13 [6400/263424 (2.41%)]\t Loss: 0.002259 Task: Phish\n",
      "Train Epoch: 13 [12800/263424 (4.83%)]\t Loss: 0.035425 Task: Phish\n",
      "Train Epoch: 13 [19200/263424 (7.26%)]\t Loss: 0.581936 Task: Multi\n",
      "Train Epoch: 13 [25600/263424 (9.69%)]\t Loss: 0.563406 Task: Multi\n",
      "Train Epoch: 13 [32000/263424 (12.12%)]\t Loss: 0.358752 Task: Multi\n",
      "Train Epoch: 13 [38400/263424 (14.55%)]\t Loss: 0.447696 Task: Multi\n",
      "Train Epoch: 13 [44800/263424 (16.98%)]\t Loss: 0.599263 Task: Multi\n",
      "Train Epoch: 13 [51200/263424 (19.41%)]\t Loss: 0.505030 Task: Multi\n",
      "Train Epoch: 13 [57600/263424 (21.84%)]\t Loss: 0.364628 Task: Multi\n",
      "Train Epoch: 13 [64000/263424 (24.27%)]\t Loss: 0.000300 Task: Advertise\n",
      "Train Epoch: 13 [70400/263424 (26.70%)]\t Loss: 0.082637 Task: Phish\n",
      "Train Epoch: 13 [76800/263424 (29.13%)]\t Loss: 0.859077 Task: Multi\n",
      "Train Epoch: 13 [83200/263424 (31.56%)]\t Loss: 0.042353 Task: Phish\n",
      "Train Epoch: 13 [89600/263424 (33.99%)]\t Loss: 0.007388 Task: Phish\n",
      "Train Epoch: 13 [96000/263424 (36.42%)]\t Loss: 0.542307 Task: Multi\n",
      "Train Epoch: 13 [102400/263424 (38.85%)]\t Loss: 0.019951 Task: Phish\n",
      "Train Epoch: 13 [108800/263424 (41.28%)]\t Loss: 0.413015 Task: Multi\n",
      "Train Epoch: 13 [115200/263424 (43.71%)]\t Loss: 0.002472 Task: Advertise\n",
      "Train Epoch: 13 [121600/263424 (46.14%)]\t Loss: 0.003281 Task: Phish\n",
      "Train Epoch: 13 [128000/263424 (48.57%)]\t Loss: 0.506877 Task: Multi\n",
      "Train Epoch: 13 [134400/263424 (51.00%)]\t Loss: 0.011832 Task: Phish\n",
      "Train Epoch: 13 [140800/263424 (53.43%)]\t Loss: 0.012430 Task: Phish\n",
      "Train Epoch: 13 [147200/263424 (55.86%)]\t Loss: 0.394902 Task: Multi\n",
      "Train Epoch: 13 [153600/263424 (58.28%)]\t Loss: 0.540970 Task: Multi\n",
      "Train Epoch: 13 [160000/263424 (60.71%)]\t Loss: 0.671443 Task: Multi\n",
      "Train Epoch: 13 [166400/263424 (63.14%)]\t Loss: 0.644202 Task: Multi\n",
      "Train Epoch: 13 [172800/263424 (65.57%)]\t Loss: 0.521004 Task: Multi\n",
      "Train Epoch: 13 [179200/263424 (68.00%)]\t Loss: 0.000704 Task: Advertise\n",
      "Train Epoch: 13 [185600/263424 (70.43%)]\t Loss: 0.409114 Task: Multi\n",
      "Train Epoch: 13 [192000/263424 (72.86%)]\t Loss: 0.059209 Task: Phish\n",
      "Train Epoch: 13 [198400/263424 (75.29%)]\t Loss: 0.004569 Task: Phish\n",
      "Train Epoch: 13 [204800/263424 (77.72%)]\t Loss: 0.005361 Task: Advertise\n",
      "Train Epoch: 13 [211200/263424 (80.15%)]\t Loss: 0.000703 Task: Advertise\n",
      "Train Epoch: 13 [217600/263424 (82.58%)]\t Loss: 0.018579 Task: Phish\n",
      "Train Epoch: 13 [224000/263424 (85.01%)]\t Loss: 0.657704 Task: Multi\n",
      "Train Epoch: 13 [230400/263424 (87.44%)]\t Loss: 0.493499 Task: Multi\n",
      "Train Epoch: 13 [236800/263424 (89.87%)]\t Loss: 0.000319 Task: Advertise\n",
      "Train Epoch: 13 [243200/263424 (92.30%)]\t Loss: 0.066257 Task: Phish\n",
      "Train Epoch: 13 [249600/263424 (94.73%)]\t Loss: 0.007264 Task: Phish\n",
      "Train Epoch: 13 [256000/263424 (97.16%)]\t Loss: 0.551959 Task: Multi\n",
      "Train Epoch: 13 [262400/263424 (99.59%)]\t Loss: 0.000790 Task: Advertise\n",
      "Test set: Task: Phish Average loss: 0.1788, Accuracy: 95.15%, Precision: 95.17%, Recall: 95.14%, F1: 95.14%\n",
      "Test set: Task: Multi Average loss: 0.8856, Accuracy: 71.15%, Precision: 71.20%, Recall: 68.24%, F1: 69.21%\n",
      "Test set: Task: Advertise Average loss: 0.0276, Accuracy: 99.55%, Precision: 99.55%, Recall: 99.55%, F1: 95.14%\n",
      "Phish: acc is: 0.9515, best acc is 0.9547\n",
      "\n",
      "Multi: acc is: 0.7115, best acc is 0.7189\n",
      "\n",
      "Advertise: acc is: 0.9955, best acc is 0.9971\n",
      "\n",
      "Train Epoch: 14 [6400/263424 (2.41%)]\t Loss: 0.010253 Task: Phish\n",
      "Train Epoch: 14 [12800/263424 (4.83%)]\t Loss: 0.022923 Task: Phish\n",
      "Train Epoch: 14 [19200/263424 (7.26%)]\t Loss: 0.623945 Task: Multi\n",
      "Train Epoch: 14 [25600/263424 (9.69%)]\t Loss: 0.588222 Task: Multi\n",
      "Train Epoch: 14 [32000/263424 (12.12%)]\t Loss: 0.281401 Task: Multi\n",
      "Train Epoch: 14 [38400/263424 (14.55%)]\t Loss: 0.427240 Task: Multi\n",
      "Train Epoch: 14 [44800/263424 (16.98%)]\t Loss: 0.572106 Task: Multi\n",
      "Train Epoch: 14 [51200/263424 (19.41%)]\t Loss: 0.507604 Task: Multi\n",
      "Train Epoch: 14 [57600/263424 (21.84%)]\t Loss: 0.345224 Task: Multi\n",
      "Train Epoch: 14 [64000/263424 (24.27%)]\t Loss: 0.000220 Task: Advertise\n",
      "Train Epoch: 14 [70400/263424 (26.70%)]\t Loss: 0.006688 Task: Phish\n",
      "Train Epoch: 14 [76800/263424 (29.13%)]\t Loss: 0.616399 Task: Multi\n",
      "Train Epoch: 14 [83200/263424 (31.56%)]\t Loss: 0.002789 Task: Phish\n",
      "Train Epoch: 14 [89600/263424 (33.99%)]\t Loss: 0.081477 Task: Phish\n",
      "Train Epoch: 14 [96000/263424 (36.42%)]\t Loss: 0.456820 Task: Multi\n",
      "Train Epoch: 14 [102400/263424 (38.85%)]\t Loss: 0.039309 Task: Phish\n",
      "Train Epoch: 14 [108800/263424 (41.28%)]\t Loss: 0.495686 Task: Multi\n",
      "Train Epoch: 14 [115200/263424 (43.71%)]\t Loss: 0.004522 Task: Advertise\n",
      "Train Epoch: 14 [121600/263424 (46.14%)]\t Loss: 0.025977 Task: Phish\n",
      "Train Epoch: 14 [128000/263424 (48.57%)]\t Loss: 0.534794 Task: Multi\n",
      "Train Epoch: 14 [134400/263424 (51.00%)]\t Loss: 0.006706 Task: Phish\n",
      "Train Epoch: 14 [140800/263424 (53.43%)]\t Loss: 0.026752 Task: Phish\n",
      "Train Epoch: 14 [147200/263424 (55.86%)]\t Loss: 0.446164 Task: Multi\n",
      "Train Epoch: 14 [153600/263424 (58.28%)]\t Loss: 0.530407 Task: Multi\n",
      "Train Epoch: 14 [160000/263424 (60.71%)]\t Loss: 0.610067 Task: Multi\n",
      "Train Epoch: 14 [166400/263424 (63.14%)]\t Loss: 0.557316 Task: Multi\n",
      "Train Epoch: 14 [172800/263424 (65.57%)]\t Loss: 0.464384 Task: Multi\n",
      "Train Epoch: 14 [179200/263424 (68.00%)]\t Loss: 0.000321 Task: Advertise\n",
      "Train Epoch: 14 [185600/263424 (70.43%)]\t Loss: 0.382883 Task: Multi\n",
      "Train Epoch: 14 [192000/263424 (72.86%)]\t Loss: 0.060854 Task: Phish\n",
      "Train Epoch: 14 [198400/263424 (75.29%)]\t Loss: 0.023672 Task: Phish\n",
      "Train Epoch: 14 [204800/263424 (77.72%)]\t Loss: 0.000797 Task: Advertise\n",
      "Train Epoch: 14 [211200/263424 (80.15%)]\t Loss: 0.054083 Task: Advertise\n",
      "Train Epoch: 14 [217600/263424 (82.58%)]\t Loss: 0.021996 Task: Phish\n",
      "Train Epoch: 14 [224000/263424 (85.01%)]\t Loss: 0.641417 Task: Multi\n",
      "Train Epoch: 14 [230400/263424 (87.44%)]\t Loss: 0.432917 Task: Multi\n",
      "Train Epoch: 14 [236800/263424 (89.87%)]\t Loss: 0.000475 Task: Advertise\n",
      "Train Epoch: 14 [243200/263424 (92.30%)]\t Loss: 0.077216 Task: Phish\n",
      "Train Epoch: 14 [249600/263424 (94.73%)]\t Loss: 0.030648 Task: Phish\n",
      "Train Epoch: 14 [256000/263424 (97.16%)]\t Loss: 0.482385 Task: Multi\n",
      "Train Epoch: 14 [262400/263424 (99.59%)]\t Loss: 0.001412 Task: Advertise\n",
      "Test set: Task: Phish Average loss: 0.1627, Accuracy: 95.45%, Precision: 95.47%, Recall: 95.46%, F1: 95.45%\n",
      "Test set: Task: Multi Average loss: 0.9129, Accuracy: 70.99%, Precision: 71.01%, Recall: 68.14%, F1: 68.93%\n",
      "Test set: Task: Advertise Average loss: 0.0189, Accuracy: 99.71%, Precision: 99.71%, Recall: 99.71%, F1: 95.45%\n",
      "Phish: acc is: 0.9545, best acc is 0.9547\n",
      "\n",
      "Multi: acc is: 0.7099, best acc is 0.7189\n",
      "\n",
      "Advertise: acc is: 0.9971, best acc is 0.9971\n",
      "\n",
      "Train Epoch: 15 [6400/263424 (2.41%)]\t Loss: 0.020532 Task: Phish\n",
      "Train Epoch: 15 [12800/263424 (4.83%)]\t Loss: 0.027363 Task: Phish\n",
      "Train Epoch: 15 [19200/263424 (7.26%)]\t Loss: 0.613129 Task: Multi\n",
      "Train Epoch: 15 [25600/263424 (9.69%)]\t Loss: 0.590688 Task: Multi\n",
      "Train Epoch: 15 [32000/263424 (12.12%)]\t Loss: 0.226204 Task: Multi\n",
      "Train Epoch: 15 [38400/263424 (14.55%)]\t Loss: 0.435639 Task: Multi\n",
      "Train Epoch: 15 [44800/263424 (16.98%)]\t Loss: 0.573953 Task: Multi\n",
      "Train Epoch: 15 [51200/263424 (19.41%)]\t Loss: 0.519459 Task: Multi\n",
      "Train Epoch: 15 [57600/263424 (21.84%)]\t Loss: 0.352368 Task: Multi\n",
      "Train Epoch: 15 [64000/263424 (24.27%)]\t Loss: 0.000309 Task: Advertise\n",
      "Train Epoch: 15 [70400/263424 (26.70%)]\t Loss: 0.008724 Task: Phish\n",
      "Train Epoch: 15 [76800/263424 (29.13%)]\t Loss: 0.618845 Task: Multi\n",
      "Train Epoch: 15 [83200/263424 (31.56%)]\t Loss: 0.095045 Task: Phish\n",
      "Train Epoch: 15 [89600/263424 (33.99%)]\t Loss: 0.005439 Task: Phish\n",
      "Train Epoch: 15 [96000/263424 (36.42%)]\t Loss: 0.408502 Task: Multi\n",
      "Train Epoch: 15 [102400/263424 (38.85%)]\t Loss: 0.029853 Task: Phish\n",
      "Train Epoch: 15 [108800/263424 (41.28%)]\t Loss: 0.373218 Task: Multi\n",
      "Train Epoch: 15 [115200/263424 (43.71%)]\t Loss: 0.019021 Task: Advertise\n",
      "Train Epoch: 15 [121600/263424 (46.14%)]\t Loss: 0.011675 Task: Phish\n",
      "Train Epoch: 15 [128000/263424 (48.57%)]\t Loss: 0.575211 Task: Multi\n",
      "Train Epoch: 15 [134400/263424 (51.00%)]\t Loss: 0.049113 Task: Phish\n",
      "Train Epoch: 15 [140800/263424 (53.43%)]\t Loss: 0.020629 Task: Phish\n",
      "Train Epoch: 15 [147200/263424 (55.86%)]\t Loss: 0.401040 Task: Multi\n",
      "Train Epoch: 15 [153600/263424 (58.28%)]\t Loss: 0.583690 Task: Multi\n",
      "Train Epoch: 15 [160000/263424 (60.71%)]\t Loss: 0.530944 Task: Multi\n",
      "Train Epoch: 15 [166400/263424 (63.14%)]\t Loss: 0.546421 Task: Multi\n",
      "Train Epoch: 15 [172800/263424 (65.57%)]\t Loss: 0.453135 Task: Multi\n",
      "Train Epoch: 15 [179200/263424 (68.00%)]\t Loss: 0.000392 Task: Advertise\n",
      "Train Epoch: 15 [185600/263424 (70.43%)]\t Loss: 0.368698 Task: Multi\n",
      "Train Epoch: 15 [192000/263424 (72.86%)]\t Loss: 0.068700 Task: Phish\n"
     ]
    }
   ],
   "source": [
    "best_acc = {\"Phish\":0.0, \"Multi\":0.0, \"Advertise\":0.0}\n",
    "NUM_EPOCHS = 20\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train(model, DEVICE, train_loader, optimizer, epoch)\n",
    "    acc, precision, recall, f1 = validation(model, DEVICE, val_loader)\n",
    "    if best_acc[\"Phish\"] < acc[\"Phish\"]:\n",
    "        task_name = \"Phish\"\n",
    "        PATH = '/hy-tmp/modelx_MTL_BERT_{}.pth'.format(task_name)\n",
    "        best_acc[\"Phish\"] = acc[\"Phish\"]\n",
    "        torch.save(model.state_dict(), PATH)\n",
    "    if best_acc[\"Multi\"] < acc[\"Multi\"]:\n",
    "        task_name = \"Multi\"\n",
    "        PATH = '/hy-tmp/modelx_MTL_BERT_{}.pth'.format(task_name)\n",
    "        best_acc[\"Multi\"] = acc[\"Multi\"]\n",
    "        torch.save(model.state_dict(), PATH)\n",
    "    if best_acc[\"Advertise\"] < acc[\"Advertise\"]:\n",
    "        task_name = \"Advertise\"\n",
    "        PATH = '/hy-tmp/modelx_MTL_BERT_{}.pth'.format(task_name)\n",
    "        best_acc[\"Advertise\"] = acc[\"Advertise\"]\n",
    "        torch.save(model.state_dict(), PATH)\n",
    "    print(\"Phish: acc is: {:.4f}, best acc is {:.4f}\\n\".format(acc[\"Phish\"], best_acc[\"Phish\"]))\n",
    "    print(\"Multi: acc is: {:.4f}, best acc is {:.4f}\\n\".format(acc[\"Multi\"], best_acc[\"Multi\"]))\n",
    "    print(\"Advertise: acc is: {:.4f}, best acc is {:.4f}\\n\".format(acc[\"Advertise\"], best_acc[\"Advertise\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
